\section{Convex Optimization}
\subsection{Convexity}
\subsubsection{Convex Sets}
A set $C$ is convex if, for any $x,y \in C$ and $\alpha \in \mathbb{R}$ with
$0 \leq \alpha \leq 1$, we have
\begin{equation}
  \alpha x + (1-\alpha) y \in C
\end{equation}
This point is known as a convex combination of $x$ and $y$.
\subsubsection{Convex Functions}
A function
$f: \mathbb{R}^n \to \mathbb{R}$ is convex if its domain $\mathcal{D}(f)$ is
a convex set, and if, for all $x, y \in \mathcal{D}(f)$ and
$\alpha \in \mathbb{R}$, $0 \leq \alpha \leq 1$, we have:
\begin{equation}
  f(\alpha x + (1-\alpha) y) \leq \alpha f(x) + (1-\alpha) f(y)
\end{equation}
A function is strictly convex if:
\begin{equation}
  f(\alpha x + (1-\alpha) y) < \alpha f(x) + (1-\alpha) f(y)
\end{equation}
\subsubsection{First-Order Conditions}
Suppose a function $f: \mathbb{R}^n \to \mathbb{R}$ is differentiable, then
$f$ is convex if and only if $\mathcal{D}(f)$ is a convex set and for all
$x, y \in \mathcal{D}(f)$ we have:
\begin{equation}
  f(y) \geq f(x) + \nabla_xf(x)^T(y-x)
\end{equation}
This is called the first-order approximation to the function $f$ at point $x$.
This is approximating $f$ with its tangent line at point $x$.
The first order condition for convexity says that $f$ is convex if and only if
the tangent line is a global underestimator of the function $f$.
In other words, if we take our function and draw a tangent line at any point,
then every point on this line will lie below the corresponding point on $f$.
\subsubsection{Second-Order Conditions}
Suppose a function $f: \mathbb{R}^n \to \mathbb{R}$ is twice differentiable,
i.e. the hessian $\nabla^2_xf(x)$ is defined for all points $x$ in the domain
of $f$. Then $f$ is convex if and only if $\mathcal{D}(f)$ is a convex set and
its hessian is PSD:
\begin{equation}
  \nabla^2_xf(y) \succeq 0
\end{equation}
\subsubsection{Jensen's Inequality}
Suppose we start with the inequality in the basic definition of a convex function:
\begin{equation}
  f(\alpha x + (1-\alpha) y) \leq \alpha f(x) + (1-\alpha) f(y)
\end{equation}
We can extend this to convex combinations of more than one point:
\begin{equation}
  f\left(\sum_{i=1}^k \alpha_i x_i\right) \leq \sum^k_{i=1} \alpha_i f(x_i)
\end{equation}
For $\sum_k \alpha = 1$ and all $\alpha \geq 0$. This can be extended to
integrals:
\begin{equation}
  f \left( \int p ( x ) x d x \right) \leq \int p ( x ) f ( x ) dx
\end{equation}
For $\int p(x)dx = 1$ and $p(x) \geq 0 \forall x$. This implies $p(x)$ is a
probability density, and we can write our inequality in terms of expectations:
\begin{equation}
  f ( \mathbf { E } [ x ] ) \leq \mathbf { E } [ f ( x ) ]
\end{equation}
And is known as Jensen's inequality.
\subsubsection{Sublevel Sets}
Given a convex function $f:\mathbb{R}^n \to \mathbb{R}$ and a real number
$\alpha \in \mathbb{R}$, the $\alpha$-sublevel set is:
\begin{equation}
  \{x \in \mathcal{D}(f): f(x) \leq \alpha\}
\end{equation}
In other words, the $\alpha$-sublevel set is the set of all points $x$
such that $f(x) \leq \alpha$.
\subsection{Convex Optimization}
A convex optimization problem seeks to optimize a convex function $f$ with
respect to a variable $x$ and possible constraints.
\begin{equation}
  \begin{array} { l l } { \text { minimize } } & { f ( x ) } \\ { \text { subject to } } & { g _ { i } ( x ) \leq 0 , \quad i = 1 , \ldots , m } \\ { } & { h _ { i } ( x ) = 0 , \quad i = 1 , \ldots , p } \end{array}
\end{equation}
where $f$ is a convex function, $g_i$ are convex functions, and $h_i$ are affine
functions. The optimal value $p^*$ is equal to the minimum possible value of
the objective function in the feasible region:
\begin{equation}
  p ^ { \star } = \min \left\{ f ( x ) : g _ { i } ( x ) \leq 0 , i = 1 , \ldots , m , h _ { i } ( x ) = 0 , i = 1 , \ldots , p \right\}
\end{equation}
The optimal point $x^*$ is a point such that $f(x^*) = p^*$.
\subsubsection{Global Optimality}
A point $x$ is locally optimal if it is feasible and if there exists some
$R > 0$ such that all feasible points $z$ with $||x-z||_2 \leq R$, satisfy
$f(x) \leq f(z)$. This means that for $x$ to be locally optimal, there
exists no nerby points that have a lower objective value. A point $x$ is globally
optimal if it is feadible and for all feasible points $z$, $f(x) \leq f(z)$.
For a convex optimization problem, all locally optimal points are globally
optimal.
\subsubsection{Gradient Descent}
Using $\alpha \in \mathbb{R}$ as the learning rate, we can update a set of
parameters $\theta$ with respect to minimizing a function $f$ as follows:
\begin{equation}
  \theta := \theta-\alpha\nabla f(\theta)
\end{equation}
Stochastic gradient descent (SGD) is updating the parameter based on each
training example, and batch gradient descent is on a batch of training
examples.
\subsubsection{Newton's Algorithm}
Newton's algorithm is a numerical method using information from the second
derivative to find $\theta$ such that $f'(\theta)=0$.
\begin{equation}
  \theta := \theta-\frac{f'(\theta)}{f''(\theta)}
\end{equation}
For multidimensional parameters:
\begin{equation}
  \theta := \theta-\alpha H^{-1} \nabla_\theta f(\theta)
\end{equation}
Where $H$ is the hessian matrix of second partial derivatives.
\begin{equation}
  H_{ij} = \frac { \partial ^ { 2 } f ( \theta ) } { \partial \theta _ { i } \partial \theta _ { j } }
\end{equation}
\subsection{Lagrange Duality and KKT Conditions}
Generic differentiable convex optimization problems are of the form:
\begin{equation}
  \begin{array} { l l } { \text { minimize } } & { f ( x ) } \\ { \text { subject to } } & { g _ { i } ( x ) \leq 0 , \quad i = 1 , \ldots , m } \\ { } & { h _ { i } ( x ) = 0 , \quad i = 1 , \ldots , p } \end{array}
\end{equation}
where $x\in \mathbb{R}^n$ is the optimization variable,
$f: \mathbb{R}^n \to \mathbb{R}$ and $g_i: \mathbb{R}^n \to \mathbb{R}$
are differentiable convex functions, and $h_i: \mathbb{R}^n \to \mathbb{R}$
are affine functions.
\subsubsection{The Lagrangian}
Given a convex constrained minimization problem, the generalized lagrangian
is a function $\mathcal{L}: \mathbb{R}^n \times \mathbb{R}^m \times \mathbb{R}^p
\to \mathbb{R}$ defined as:
\begin{equation}
  \mathcal { L } ( x , \alpha , \beta ) = f ( x ) + \sum _ { i = 1 } ^ { m } \alpha _ { i } g _ { i } ( x ) + \sum _ { i = 1 } ^ { p } \beta _ { i } h _ { i } ( x )
\end{equation}
We refer to $x$ as the primal variables of the Lagrangian. The $\alpha$ and
$\beta$ are refered to as the dual variables or lagrange multipliers. The
key idea behind Lagrangian duality is for any convex optimization problem,
there always exist settings of the dual variables such that the unconstrained
minimum of the Lagrangian with respect to the primal variables (keeping the
dual variables fixed) coincides with the solution of the original constrained
minimization problem.
\subsubsection{Primal and Dual Problems}
The primal problem is:
\begin{equation}
  \min _ { x } \underbrace { \left[ \max _ { \alpha , \beta : \alpha _ { i } \geq 0 , \forall i } \mathcal { L } ( x , \alpha , \beta ) \right] } _ { \theta _ { \mathcal { P } ( x ) } } = \min _ { x } \theta _ { \mathcal { P } } ( x )
\end{equation}
Here, $\theta_{\mathcal{P}} : \mathbb{R}^n \to \mathbb{R}$ is the primal objective, and
the right hand side is the primal problem. In addition,  $p^* = \theta_{\mathcal{P}}(x^*)$
is the optimal value of the primal objective. \\
The dual problem is:
\begin{equation}
  \max _ { \alpha , \beta : \alpha _ { i } \geq 0 , \forall i  } \underbrace { \left[ \min _ { x } \mathcal { L } ( x , \alpha , \beta ) \right] } _ { \theta _ { \mathcal { D } ( x ) } } = \max _ { \alpha , \beta : \alpha _ { i } \geq 0 , \forall i  } \theta _ { \mathcal { D } } ( x )
\end{equation}
Here, $\theta_{\mathcal{D}} : \mathbb{R}^m \times \mathbb{R}^p \to \mathbb{R}$
is the dual objective, and
the right hand side is the dual problem. In addition,
$s^* = \theta_{\mathcal{D}}(\alpha^*, \beta^*)$
is the optimal value of the dual objective.
\subsubsection{Strong and Weak Duality}
Weak duality for any pair of primal and dual problems, with $d^*$ the optimal
objective value of the dual problem and $p^*$ as the optimal objective value
of the primal problem we have:
\begin{equation}
  d^* \leq p^*
\end{equation}
Strong duality is when for any pair of primal and dual problems which satisfy
certain technical conditions called constraint qualifications, then:
\begin{equation}
  d^* = p^*
\end{equation}
\subsubsection{Complementary Slackness}
If strong duality holds, then $\alpha^*_i g(x^*_i) = 0$ for each $i=1,\hdots,m$.
This property is known as complementary slackness. This implies the following:
\begin{equation}
  \alpha _ { i } ^ { * } > 0 \Longrightarrow g _ { i } \left( x ^ { * } \right) = 0
\end{equation}
\begin{equation}
  g _ { i } \left( x ^ { * } \right) < 0 \quad \Longrightarrow \quad \alpha _ { i } ^ { * } = 0
\end{equation}
\subsubsection{The KKT Conditions}
Suppose that $x^* \in \mathbb{R}^n$, $\alpha^* \in \mathbb{R}^m$ and
$\beta^* \in \mathcal{R}^p$ satisfy the following conditions:
\begin{enumerate}
  \item Primal feasibility: $g_i(x^*) \leq 0$, for $i=1,\hdots, m$ and
    $h_i(x^*)=0$ for $i=1,\hdots, p$
  \item Dual feasibility: $\alpha^*_i \geq 0$ for $i=1,\hdots, m$
  \item Complementary slackness: $\alpha^*_i g_i(x^*) = 0$ for $i=1,\hdots, m$
  \item Lagrangian stationarity: $\nabla_x \mathcal{L}(x^*, \alpha^*, \beta^*) = \vec{0}$
\end{enumerate}
Then $x^*$ is primal optimal and $(\alpha^*, \beta^*)$ are dual optimal.
Furthermore, if strong duality holds, then any primal optimal $x^*$ and dual
optimal $(\alpha^*, \beta^*)$ must satisfy the conditions $1$ through $4$.
These conditions are known as the Karush-Kuhn-Tucker (KKT) conditions.
