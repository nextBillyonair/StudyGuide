\section{Information Theory}
Information Theory revolves around quantifying how much information is present
in a signal. The basic intuition lies in the fact that learning an unlikely
event has occured is more informative than learning that a likely event has
occured. The basics are:
\begin{enumerate}
  \item Likely events should have low information content, and in the extreme
  case, events that are guaranteed to happen should have no information content
  whatsoever.
  \item Less likely events should have higher information content.
  \item Independent events should have additive information.
\end{enumerate}
We satisfy all three properties by defining self-information of an event $x$
for a probability distribution $P$ as:
\begin{equation}
  I(x) = -\log P(x)
\end{equation}
We can quantify the amount of uncertainty in a
distribution using Shannon entropy:
\begin{equation}
  H ( P ) = \mathbb { E } _ { \mathrm { x } \sim P } [ I ( x ) ] = - \mathbb { E } _ { \mathrm { x } \sim P } [ \log P ( x ) ]
\end{equation}
Which in the discrete setting is written as:
\begin{equation}
  H ( P ) = -\sum_{x} P(x) \log P(x)
\end{equation}
In other words, the Shannon entropy of a distribution is the expected amount
of information in an event drawn from that distribution. It gives a lower bound
on the number of bits needed on average to encode symbols drawn
from a distribution $P$. If we have two separate probability distributions
$P(x)$ and $Q(x)$ over the same random variable $\mathrm{x}$, we can measure how
different these two distributions are using the Kullback-Leibler (KL)
divergence:
\begin{equation}
  \begin{split}
    D _ { \mathrm { KL } } ( P \| Q ) \quad =& \quad \mathbb { E } _ { \mathbf { x } \sim P } \left[ \log \frac { P ( x ) } { Q ( x ) } \right]\\
    \\
    =& \quad \mathbb { E } _ { \mathbf { x } \sim P } \left[ \log P ( x ) - \log Q ( x ) \right] \\
    \\
    =& \quad \sum_x P(x) \frac{\log P(x)}{\log Q(x)} \\
  \end{split}
\end{equation}
In the case of discrete variables, it is the extra amount of information
needed to send a message containing symbols drawn from probability distribution
$P$, when we use a code that was designed to minimize the length of messages
drawn from probability distribution $Q$. The KL divergence is always
non-negative, and is $0$ if and only if $P$ and $Q$ are the same. We can
relate the KL divergence to cross-entropy.
\begin{equation}
  \begin{split}
    H(P, Q) \quad =& \quad H(P) + D _ { \mathrm { KL } } ( P \| Q ) \\
    \\
    =& \quad - \mathbb { E } _ { \mathbf { x } \sim P } \left[ \log Q ( x ) \right]\\
    \\
    =& \quad -\sum_x P(x) \log Q(x) \\
  \end{split}
\end{equation}
Minimizing the cross-entropy with respect to $Q$ is equivalent to
minimizing the KL divergence, because $Q$ does not participate in the
omitted term (entropy is constant).
