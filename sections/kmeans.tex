\section{K-Means Clustering}
CLustering seeks to group similiar points of data together in a cluster. We
denote $c^{(i)}$ as the cluster for data point $i$ and $\mu_j$ as the center
for cluster $j$. We denote $k$ as the number of clusters and $n$ as the
dimension of our data.
\subsection{Algorithm}
After randomly initializing the cluster centroids
$\mu_1, \mu_2, \hdots, \mu_k \in \mathbb{R}^n$, repeat until convergence:
\begin{enumerate}
  \item For every data point $i$:
    \begin{equation}
      c^{(i)}=\underset{j}{\textrm{arg min}}||x^{(i)}-\mu_j||^2
    \end{equation}
  \item For each cluster $j$:
    \begin{equation}
      \mu_j=\frac{\displaystyle\sum_{i=1}^m1_{\{c^{(i)}=j\}}x^{(i)}}{\displaystyle\sum_{i=1}^m1_{\{c^{(i)}=j\}}}
    \end{equation}
\end{enumerate}
The first step is known as cluster assignment, and the second updates the
cluster center (i.e. the average of all points in the cluster). In order to
see if it converges, use the distortion function:
\begin{equation}
  J(c,\mu)=\sum_{i=1}^m||x^{(i)}-\mu_{c^{(i)}}||^2
\end{equation}
The distortion function $J$ is non-convex, and coordinate descent of $J$ is not
guaranteed to converge to the global minimum (i.e. susceptible to local optima).
\subsection{Hierarchical Clustering}
Hierarchical clustering is a clustering algorithm with an agglomerative
hierarchical approach that builds nested clusters in a successive manner. The
types are:
\begin{enumerate}
  \item Ward Linkage: minimize within cluster distance
  \item Average Linkage: minimize average distance between cluster pairs
  \item Complete Linkage: minimize maximum distance between cluster pairs
\end{enumerate}
\subsection{Clustering Metrics}
In an unsupervised learning setting, it is often hard to assess the
performance of a model since we don't have the ground truth labels as was
the case in the supervised learning setting.
\paragraph{Silhouette coefficient} By noting $a$ and $b$ the
mean distance between a sample and all other points in the same class,
and between a sample and all other points in the next nearest cluster,
the silhouette coefficient $s$ for a single sample is defined as follows:
\begin{equation}
  s = \frac{b - a}{\max(a, b)}
\end{equation}
\paragraph{Calinskli-Harabaz Index} By noting $k$ the number of clusters, $B_k$
and $W_k$ the between and within-clustering dispersion matricies defined as:
\begin{equation}
  B_k=\sum_{j=1}^kn_{c^{(i)}}(\mu_{c^{(i)}}-\mu)(\mu_{c^{(i)}}-\mu)^T
\end{equation}
\begin{equation}
  W_k=\sum_{i=1}^m(x^{(i)}-\mu_{c^{(i)}})(x^{(i)}-\mu_{c^{(i)}})^T
\end{equation}
the Calinksli-Harabaz index $s(k)$ indicated how well a clustering model
defines its clusters, such that higher scores indicate more dense and well
separated cluster assignments. It is defined as:
\begin{equation}
  s(k)=\frac{\textrm{Tr}(B_k)}{\textrm{Tr}(W_k)}\times\frac{N-k}{k-1}
\end{equation}
