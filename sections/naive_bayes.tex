\section{Generative Learning: Naive Bayes}
\subsection{Assumptions}
The naive bayes model supposes that the features of each data point are all
independent:
\begin{equation}
  P(x|y)=P(x_1,x_2,...,x_n|y)=P(x_1|y)P(x_2|y)...P(x_n|y)=\prod_{i=1}^nP(x_i|y)
\end{equation}
\subsection{Estimation}
We can write the likelihood of the data as:
\begin{equation}
  \mathcal{L}(\phi_y, \phi_{j=l|y=k}) = \prod_{i=1}^m P(x^{(i)}, y^{(i)})
\end{equation}
with classes denoted by $k$ and features $x_j = l$ ($x_j$ takes on value $l$).
By maximizing the estimates, we get:
\begin{equation}
  \phi_y = P(y=k)=\frac{1}{m} \sum_{i=1}^m 1_{\{y^{(i)}=k\}}
\end{equation}
\begin{equation}
  \phi_{j=l|y=k} = P(x_j=l|y=k)= \frac{ \sum_{i=1}^m 1_{\{y^{(i)}=k \, \wedge \, x_j^{(i)} = l\}} }{ \sum_{i=1}^m 1_{\{y^{(i)}=k\}} }
\end{equation}
\subsubsection{Laplace Smoothing}
Laplace smoothing allows for unseen data to have a probability, and not automatically
destory the prediciton process by setting all classes to $0$. We can replace
our feature estimates with the following:
\begin{equation}
  \phi_j = \frac{ \sum_{i=1}^m 1_{\{x^{(i)}=j\}} +1} { m + k }
\end{equation}
Or more generically:
\begin{equation}
  \phi_{j=l|y=k} = \frac{ \sum_{i=1}^m 1_{\{x_j^{(i)}=l \wedge y^{(i)} = k\}} + 1 } { \sum_{i=1}^m 1_{\{y^{(i)}=k\}} + |K| }
\end{equation}
where $|K|$ is the number of classes
\subsection{Prediction}
To classify a point $x$, we find the class $y=k$ which maximizes the probability:
\begin{equation}
  \hat{y} = \underset{k}{\textrm{arg max}} \,\, P(y=k) \cdot\prod_{i=j}^n P(x_j | y=k)
\end{equation}
