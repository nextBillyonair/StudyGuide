\section{Principal Component Analysis}
Principal Component Analysis is a dimension reduction technique that finds
the variance maximizing the directions onto which to project the data.
\subsection{Eigenvalues, Eigenvectors, and the Spectral Theorem}
Recall that for a given matrix $A\in \mathbb{R}^{n \times n}$, $\lambda$ is said
to be an eigenvalue of $A$ if there exists a vector
$z \in \mathbb{R}^n \backslash \{ 0 \}$, called an eigenvector, such that:
\begin{equation}
  Az=\lambda z
\end{equation}
The spectral theorem states that given matrix $A\in \mathbb{R}^{n \times n}$, if
$A$ is symmetric then $A$ is diagonalizable by a real orthogonal matrix
$U\in \mathbb{R}^{n \times n}$. Note
$\Lambda = \textrm{diag}(\lambda_1, \hdots, \lambda_n)$. Then $\exists \, \Lambda$
such that:
\begin{equation}
  A=U\Lambda U^T
\end{equation}
Note that the eigenvector associated with the largest eigenvalue is called the
principal eigenvector of matrix $A$.
\subsection{Algorithm}
The PCA procedure projects the data onto $k$ dimensions by maximizing the variance
of the data as follows:
\begin{enumerate}
  \item Normalize the data to have mean $0$, standard deviation $1$:
    \begin{equation} x^{(i)}\leftarrow\frac{x^{(i)}-\mu}{\sigma} \end{equation}
      where $\mu$ and $\sigma^2$ are:
      \begin{equation} \mu = \frac{1}{m}\sum_{i=1}^mx^{(i)} \end{equation}
      \begin{equation} \sigma^2=\frac{1}{m}\sum_{i=1}^m(x^{(i)}-\mu)^2 \end{equation}
  \item Compute covariance matrix $\Sigma$, which is symmetric with real eigenvalues.
    \begin{equation} \Sigma = \frac{1}{m}\sum_{i=1}^m x^{(i)}x^{(i)^T} \in \mathbb{R}^{n \times n} \end{equation}
  \item Compute $u_1, \hdots, u_k \in \mathbb{R}^{n}$ the $k$ orthogonal principal
    eigenvectors of $\Sigma$, i.e. the orthogonal eigenvectors of the $k$
    largest eigenvalues.
  \item Project the data on $\textrm{span}_\mathbb{R}(u_1,...,u_k)$ to create
    a vector $y^{(i)}$ from point $x^{(i)}$:
    \begin{equation} y^{(i)} = U^T x^{(i)} = \left( \begin{array} { c } { u _ { 1 } ^ { T } x ^ { ( i ) } } \\ { u _ { 2 } ^ { T } x ^ { ( i ) } } \\ { \vdots } \\ { u _ { k } ^ { T } x ^ { ( i ) } } \end{array} \right) \in \mathbb { R } ^ { k } \end{equation}
\end{enumerate}
This procedure maximizes the variance among all $k$-dimensional spaces.
\subsection{Algorithm: SVD}
Eigenvalue decomposition is deifned for square matricies, and using the SVD of a
data matrix $X$ is often used in practice.
\begin{enumerate}
  \item Zero-mean the data to have mean $0$:
    \begin{equation} x^{(i)}\leftarrow x^{(i)}-\mu \end{equation}
      where $\mu$ is:
      \begin{equation} \mu = \frac{1}{m}\sum_{i=1}^mx^{(i)} \end{equation}
  \item Computed the singular value decomposition of $X_{\mu}$, our zero-meaned
    data matrix:
    \begin{equation}
      X_{\mu} = USV
    \end{equation}
  \item Take the first $k$ columns of $U$ as our transform matrix, denoted $U_k$.
  \item Project the data to create a matrix $Y$ from data matrix $X_{\mu}$:
    \begin{equation} Y = X U_k\end{equation}
\end{enumerate}
