%------------------------------------------------------------------------------
%   PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%------------------------------------------------------------------------------

\documentclass[twoside,twocolumn]{article}

%\usepackage[sc]{mathpazo} % Use the Palatino font
%\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
%\linespread{1.05} % Line spacing - Palatino needs more space between lines
%\usepackage{microtype} % Slightly tweak font spacing for aesthetics

\usepackage[english]{babel} % Language hyphenation and typographical rules

% Document margins
\usepackage[hmarginratio=1:1,top=32mm,left=20mm,right=20mm,columnsep=20pt]{geometry}
% Custom captions under/above floats in tables or figures
\usepackage[hang, small,labelfont=bf,up,textfont=it,up]{caption}
\usepackage{booktabs} % Horizontal rules in tables

\usepackage{enumitem} % Customized lists
\setlist[itemize]{noitemsep} % Make itemize lists more compact
\usepackage{textcomp}

% Allows abstract customization
\usepackage{abstract}
% Set the "Abstract" text to bold
\renewcommand{\abstractnamefont}{\normalfont\bfseries}
% Set the abstract itself to small italic text
\renewcommand{\abstracttextfont}{\normalfont\small\itshape}

\usepackage{fancyhdr} % Headers and footers
\pagestyle{fancy} % All pages have headers and footers
\fancyhead{} % Blank out the default header
\fancyfoot{} % Blank out the default footer
\fancyhead[C]{\thetitle}
\fancyfoot[RO,LE]{\thepage} % Custom footer text

\usepackage{titling} % Customizing the title section

\usepackage{hyperref} % For hyperlinks in the PDF
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{tikz}
\usetikzlibrary{bayesnet, arrows, positioning, fit, arrows.meta, shapes}

\usepackage{color}
\usepackage{caption}
\usepackage{subcaption}

\usepackage{graphicx}


\captionsetup[figure]{labelfont={bf},textfont=normalfont}

%------------------------------------------------------------------------------
%   TITLE SECTION
%------------------------------------------------------------------------------
\newlength\mystoreparindent
\newenvironment{myparindent}[1]{%
  \setlength{\mystoreparindent}{\the\parindent}
  \setlength{\parindent}{#1}
  }{%
  \setlength{\parindent}{\mystoreparindent}
}

\setlength{\droptitle}{-4\baselineskip} % Move the title up

\pretitle{\begin{center}\Huge\bfseries} % Article title formatting
\posttitle{\end{center}} % Article title closing formatting

\title{Machine Learning Study Guide}
\author{%
  \textsc{William Watson} \\[1ex]
  \normalsize Johns Hopkins University \\
  \normalsize \href{mailto:billwatson@jhu.edu}{billwatson@jhu.edu}
}

\date{}%\today} % Leave empty to omit a date
% \renewcommand{\maketitlehookd}{%

% }

%------------------------------------------------------------------------------

\begin{document}

% Print the title
\maketitle

%------------------------------------------------------------------------------
%   ARTICLE CONTENTS
%------------------------------------------------------------------------------

%------------------------------------------------
\onecolumn
%------------------------------------------------
\section{Linear Algebra and Calculus}
\subsection{General Notation}
\subsection{Matrix Operations}
\subsection{Matrix Properties}
\subsection{Matrix Calculus}
%------------------------------------------------
\section{Convex Optimization}
\subsection{Convexity}
\subsection{Convex Optimization}
\subsubsection{Gradient Descent}
\subsubsection{Newton's Algorithm}
\subsection{Lagrange Duality and KKT Conditions}
%------------------------------------------------
\section{Probability and Statistics}
\subsection{Basics}
\subsection{Conditional Probability}
\subsection{Random Variables}
\subsection{Jointly Distributed Random Variables}
\subsection{Parameter Estimation}
\subsection{Probability Bounds and Inequalities}
\section{Information Theory}
Information Theory revolves around quantifying how much information is present
in a signal. The basic intuition lies in the fact that learning an unlikely
event has occured is more informative than learning that a likely event has
occured. The basics are:
\begin{enumerate}
  \item Likely events should have low information content, and in the extreme
  case, events that are guaranteed to happen should have no information content
  whatsoever.
  \item Less likely events should have higher information content.
  \item Independent events should have additive information.
\end{enumerate}
We satisfy all three properties by defining self-information of an event $x$
for a probability distribution $P$ as:
\begin{equation}
  I(x) = -\log P(x)
\end{equation}
We can quantify the amount of uncertainty in a distribution using Shannon
Entropy:
\begin{equation}
  H ( P ) = \mathbb { E } _ { \mathrm { x } \sim P } [ I ( x ) ] = - \mathbb { E } _ { \mathrm { x } \sim P } [ \log P ( x ) ]
\end{equation}
Which in the discrete setting is written as:
\begin{equation}
  H ( P ) = -\sum_{x} P(x) \log P(x)
\end{equation}
In other words, the Shannon entropy of a distribution is the expected amount
of information in an event drawn from that distribution. It gives a lower bound
on the number of bits needed on average to encode symbols drawn
from a distribution $P$. If we have two separate probability distributions
$P(x)$ and $Q(x)$ over the same random variable $\mathrm{x}$, we can measure how
different these two distributions are using the Kullback-Leibler (KL)
divergence:
\begin{equation}
  \begin{split}
    D _ { \mathrm { KL } } ( P \| Q ) \quad =& \quad \mathbb { E } _ { \mathbf { x } \sim P } \left[ \log \frac { P ( x ) } { Q ( x ) } \right]\\
    \\
    =& \quad \mathbb { E } _ { \mathbf { x } \sim P } \left[ \log P ( x ) - \log Q ( x ) \right] \\
    \\
    =& \quad \sum_x P(x) \frac{\log P(x)}{\log Q(x)} \\
  \end{split}
\end{equation}
In the case of discrete variables, it is the extra amount of information
needed to send a message containing symbols drawn from probability distribution
$P$, when we use a code that was designed to minimize the length of messages
drawn from probability distribution $Q$. The KL divergence is always
non-negative, and is $0$ if and only if $P$ and $Q$ are the same. We can
relate the KL divergence to cross-entropy.
\begin{equation}
  \begin{split}
    H(P, Q) \quad =& \quad H(P) + D _ { \mathrm { KL } } ( P \| Q ) \\
    \\
    =& \quad - \mathbb { E } _ { \mathbf { x } \sim P } \left[ \log Q ( x ) \right]\\
    \\
    =& \quad -\sum_x P(x) \log Q(x) \\
  \end{split}
\end{equation}
Minimizing the cross-entropy with respect to $Q$ is equivalent to
minimizing the KL divergence, because $Q$ does not participate in the
omitted term (entropy is constant).
%------------------------------------------------
\section{Machine Learning Basics} %Or learning theory?
\subsection{Notation}
\subsection{Types of Learning}
\subsection{Metrics}
\subsubsection{Classification}
\subsubsection{Regression}
\subsection{Bias and Variance}
%------------------------------------------------
\section{Linear Regression}
Linear Regression seeks to approximate a real valued label $y$ as a
linear function of $x$:
\begin{equation}
  h_{\theta}(x) = \theta_0 + \theta_1 \cdot x_1 + \cdots + \theta_n \cdot x_n
\end{equation}
The $\theta_i$'s are the parameters, or weights. If we include the
intercept term via $x_0=1$, we can write our model more compactly as:
\begin{equation}
  h(x) = \sum_{i=0}^{n} \theta_i \cdot x_i = \theta^T x
\end{equation}
Here $n$ is the number of input variables, or features. In Linear Regression,
we seek to make $h(x)$ as close to $y$ for a set of training examples. We
define the cost function as:
\begin{equation}
  J(\theta) = \frac{1}{2} \sum_{i=1}^{m} \left( h \left(x^{(i)}\right) - y^{(i)}\right)^2
\end{equation}
\subsection{LMS Algorithm}
We seek to find a set of $\theta$ such that we minimize $J(\theta)$ via a
search algorithm that starts at some initial guess for our parameters and takes
incremental steps to make $J(\theta)$ smaller until convergence. This is know
as gradient descent:
\begin{equation}
  \theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j}J(\theta)
\end{equation}
Here, $\alpha$ is the learning rate. We can derive the partial derivative as:
\begin{equation}
  \begin{split}
    \frac{\partial}{\partial \theta_{j}}J(\theta) \quad =& \quad \frac{\partial}{\partial \theta_{j}}\frac{1}{2} \left( h(x)-y \right)^{2} \\
    =& \quad 2\cdot \frac{1}{2} \left(h(x) - y \right) \cdot \frac{\partial}{\partial \theta_{j}} (h(x) - y) \\
    =& \quad \left(h(x) - y \right) \cdot \frac{\partial}{\partial \theta_{j}} \left(\sum_{i=0}^{n} \theta_{i} x_{i} - y \right) \\
    =& \quad \left( h ( x ) - y \right) x _ { j } \\
\end{split}
\end{equation}
Hence, for a single example (stochastic gradient descent):
\begin{equation}
  \theta _ { j } : = \theta _ { j } + \alpha \left( y ^ { ( i ) } - h \left( x ^ { ( i ) } \right) \right) x _ { j } ^ { ( i ) }
\end{equation}
This is called the LMS update rule. For a batched version, we can evaluate the
gradient on a set of examples (batch gradient descent), or the full set
(gradient descent).
\begin{equation}
  \theta _ { j } : = \theta _ { j } + \alpha \sum _ { i = 1 } ^ { m } \left( y ^ { ( i ) } - h \left( x ^ { ( i ) } \right) \right) x _ { j } ^ { ( i ) }
\end{equation}
\subsection{The Normal Equations}
We can also directly minimize $J$ without using an iterative algorithm. We
define $X$ as the matrix of all samples of size $m$ by $n$.
We let $\vec{y}$ be a $m$ dimensional vector of all target values. We can
define our cost function $J$ as:
\begin{equation}
  J(\theta) = \frac { 1 } { 2 } ( X \theta - \vec { y } ) ^ { T } ( X \theta - \vec { y } ) = \frac { 1 } { 2 } \sum _ { i = 1 } ^ { m } \left( h \left( x ^ { ( i ) } \right) - y ^ { ( i ) } \right) ^ { 2 }
\end{equation}
We then take the derivative and find its roots.

\begin{equation}
  \begin{split}
    \nabla _ { \theta } J ( \theta ) \quad =& \quad \nabla _ { \theta } \frac { 1 } { 2 } ( X \theta - \vec { y } ) ^ { T } ( X \theta - \vec { y } ) \\
    =& \quad \frac { 1 } { 2 } \nabla _ { \theta } \left( \theta ^ { T } X ^ { T } X \theta - \theta ^ { T } X ^ { T } \vec { y } - \vec { y } ^ { T } X \theta + \vec { y } ^ { T } \vec { y } \right) \\
    =& \quad \frac { 1 } { 2 } \nabla _ { \theta } \left( \operatorname { tr } \theta ^ { T } X ^ { T } X \theta - 2 \operatorname { tr } \vec { y } ^ { T } X \theta \right) \\
    =& \quad \frac { 1 } { 2 } \left( X ^ { T } X \theta + X ^ { T } X \theta - 2 X ^ { T } \vec { y } \right) \\
    =& \quad X ^ { T } X \theta - X ^ { T } \vec { y } \\
  \end{split}
\end{equation}
To minimize $J$, we set its derivatives to zero, and obtain the normal
equations:
\begin{equation}
  X ^ { T } X \theta = X ^ { T } \vec { y }
\end{equation}
Which solves $\theta$ for a value that minimizes $J(\theta)$ in closed form:
\begin{equation}
  \theta = \left( X ^ { T } X \right) ^ { - 1 } X ^ { T } \vec { y }
\end{equation}
\subsection{Probabilistic Interpretation}
Why does linear regression use the least-squares cost function? Assume that
the target variables and inputs are related via:
\begin{equation}
  y ^ { ( i ) } = \theta ^ { T } x ^ { ( i ) } + \epsilon ^ { ( i ) }
\end{equation}
Here, $\epsilon^{(i)}$ is an error term for noise. We assume each
$\epsilon^{(i)}$ is independently and identically distributed according to a
Gaussian distribution with mean zero and some variance $\sigma^2$. Hence,
$\epsilon^{(i)} \sim \mathcal { N } \left( 0 , \sigma ^ { 2 } \right)$, so
the density for any sample $x^{(i)}$ with label $y^{(i)}$ is
$y^{(i)} | x^{(i)}; \theta \sim \mathcal{N} \left( \theta^T x^{(i)}, \sigma^{2} \right)$.
This implies:
\begin{equation}
  p \left( y ^ { ( i ) } | x ^ { ( i ) } ; \theta \right) = \frac { 1 } { \sqrt { 2 \pi } \sigma } \exp \left( - \frac { \left( y ^ { ( i ) } - \theta ^ { T } x ^ { ( i ) } \right) ^ { 2 } } { 2 \sigma ^ { 2 } } \right)
\end{equation}
The probability of a dataset $X$ is quantified by a likelihood function:
\begin{equation}
  L ( \theta ) = L ( \theta ; X , \vec { y } ) = p ( \vec { y } | X ; \theta )
\end{equation}
Since we assume independence on each noise term (and samples), we can write
the likelihood function as:
\begin{equation}
  \begin{aligned}
    L ( \theta ) & = \prod _ { i = 1 } ^ { m } p \left( y ^ { ( i ) } | x ^ { ( i ) } ; \theta \right) \\
    & = \prod _ { i = 1 } ^ { m } \frac { 1 } { \sqrt { 2 \pi } \sigma } \exp \left( - \frac { \left( y ^ { ( i ) } - \theta ^ { T } x ^ { ( i ) } \right) ^ { 2 } } { 2 \sigma ^ { 2 } } \right)
  \end{aligned}
\end{equation}
To get the best choice of parameters $\theta$, we perform maximum likelihood
estimation such that $L(\theta)$ is maximized. Usually we take the negative log
and minimize:
\begin{equation}
  \begin{aligned}
    \ell ( \theta ) & = - \log L ( \theta ) \\
    & = - \log \prod _ { i = 1 } ^ { m } \frac { 1 } { \sqrt { 2 \pi } \sigma } \exp \left( - \frac { \left( y ^ { ( i ) } - \theta ^ { T } x ^ { ( i ) } \right) ^ { 2 } } { 2 \sigma ^ { 2 } } \right) \\
    & = - \sum _ { i = 1 } ^ { m } \log \frac { 1 } { \sqrt { 2 \pi } \sigma } \exp \left( - \frac { \left( y ^ { ( i ) } - \theta ^ { T } x ^ { ( i ) } \right) ^ { 2 } } { 2 \sigma ^ { 2 } } \right) \\
    & = - m \log \frac { 1 } { \sqrt { 2 \pi } \sigma } + \frac { 1 } { \sigma ^ { 2 } } \cdot \frac { 1 } { 2 } \sum _ { i = 1 } ^ { m } \left( y ^ { ( i ) } - \theta ^ { T } x ^ { ( i ) } \right) ^ { 2 }
  \end{aligned}
\end{equation}
Hence, maximizing $L(\theta$) is the same as minimizing the negative log
likelihood $\ell(\theta)$, which for linear regression is the least squares
cost function:
\begin{equation}
  \frac { 1 } { 2 } \sum _ { i = 1 } ^ { m } \left( y ^ { ( i ) } - \theta ^ { T } x ^ { ( i ) } \right) ^ { 2 }
\end{equation}
Under the previous probabilistic assumptions on the data, least-squares
regression corresponds to finding the maximum likelihood estimate of $\theta$.
This is thus one set of assumptions under which least-squares regression
can be justified as performing maximum likelihood estimation. Note that $\theta$
is independent of $\sigma^2$.
\subsection{Locally Weighted Linear Regression}
Locally Weighted Regression, also known as LWR, is a variant of
linear regression that weights each training example in its cost function by
$w^{(i)}(x)$, which is defined with parameter $\tau \in \mathbb{R}$ as:
\begin{equation}
  w^{(i)}(x)=\exp\left(-\frac{(x^{(i)}-x)^2}{2\tau^2}\right)
\end{equation}
Hence, in LWR, we do the following:
\begin{enumerate}
  \item Fit $\theta$ to minimize $\sum_i w^{(i)} \left( y^{(i)} - \theta^{T} x^{(i)} \right)^{2}$
  \item Output $\theta^T x$
\end{enumerate}
This is a non-parametric algorithm, where non-parametric refers to the fact
that the amount of information we need to represent the hypothesis $h$ grows
linearly with the size of the training set.
%------------------------------------------------
\section{Logistic Regression}
We can extend this learning to classification problems, where we have binary
labels $y$ that are either $0$ or $1$.
\subsection{The Logistic Function}
For logistic regression, our new hypothesis for estimating the class of a
sample $x$ is:
\begin{equation}
  h( x ) = g \left( \theta ^ { T } x \right) = \frac { 1 } { 1 + e ^ { - \theta ^ { T } x } }
\end{equation}
where $g(z)$ is the logistic or sigmoid function:
\begin{equation}
  g ( z ) = \frac { 1 } { 1 + e ^ { - z } }
\end{equation}
The sigmoid function is bounded between $0$ and $1$, and tends towards $1$ as
$z \rightarrow \infty$. It tends towards $0$ when $z \rightarrow -\infty$.
A useful property of the sigmoid function is the form of its derivative:
\begin{equation}
  \begin{aligned} g ^ { \prime } ( z ) \quad & = \quad \frac { d } { d z } \frac { 1 } { 1 + e ^ { - z } } \\
    & = \quad \frac { 1 } { \left( 1 + e ^ { - z } \right) ^ { 2 } } \left( e ^ { - z } \right) \\
    & = \quad \frac { 1 } { \left( 1 + e ^ { - z } \right) } \cdot \left( 1 - \frac { 1 } { \left( 1 + e ^ { - z } \right) } \right) \\
    & = \quad g ( z ) ( 1 - g ( z ) )
  \end{aligned}
\end{equation}
\subsection{Cost Function}
To fit $\theta$ for a set of training examples, we assume that:
\begin{equation}
  \begin{aligned}
    P ( y = 1 | x ; \theta ) & = h( x ) \\
    P ( y = 0 | x ; \theta ) & = 1 - h( x )
  \end{aligned}
\end{equation}
This can be written more compactly as:
\begin{equation}
  p ( y | x ; \theta ) = \left( h ( x ) \right) ^ { y } \left( 1 - h ( x ) \right) ^ { 1 - y }
\end{equation}
Assume $m$ training examples generated independently, we define the likelihood
function of the parameters as:
\begin{equation}
  \begin{aligned}
    L ( \theta ) \quad & = \quad p _ { m } ^ { m } | X ; \theta ) \\
    & = \quad \prod _ { i = 1 } ^ { m } p \left( y ^ { ( i ) } | x ^ { ( i ) } ; \theta \right) \\
    & = \quad \prod _ { i = 1 } ^ { m } \left( h \left( x ^ { ( i ) } \right) \right) ^ { y ^ { ( i ) } } \left( 1 - h  \left( x ^ { ( i ) } \right) \right) ^ { 1 - y ^ { ( i ) } }
  \end{aligned}
\end{equation}
And taking the negative log likelihood to minimize:
\begin{equation}
  \begin{aligned}
    \ell ( \theta ) \quad & = \quad -\log L ( \theta ) \\
    & = \quad -\sum _ { i = 1 } ^ { m } y ^ { ( i ) } \log h \left( x ^ { ( i ) } \right) + \left( 1 - y ^ { ( i ) } \right) \log \left( 1 - h \left( x ^ { ( i ) } \right) \right)
  \end{aligned}
\end{equation}
This is known as the binary cross-entropy loss function.
\subsection{Gradient Descent}
Let’s start by working with just one training example (x,y), and take
derivatives to derive the stochastic gradient ascent rule:
\begin{equation}
  \begin{aligned}
    \frac { \partial } { \partial \theta _ { j } } \ell ( \theta ) \quad & = \quad - \left( y \frac { 1 } { g \left( \theta ^ { T } x \right) } - ( 1 - y ) \frac { 1 } { 1 - g \left( \theta ^ { T } x \right) } \right) \frac { \partial } { \partial \theta _ { j } } g \left( \theta ^ { T } x \right) \\
    & = \quad - \left( y \frac { 1 } { g \left( \theta ^ { T } x \right) } - ( 1 - y ) \frac { 1 } { 1 - g \left( \theta ^ { T } x \right) } \right) g \left( \theta ^ { T } x \right) \left( 1 - g \left( \theta ^ { T } x \right) \right) \frac { \partial } { \partial \theta _ { j } } \theta ^ { T } x \\
    & = \quad - \left( y \left( 1 - g \left( \theta ^ { T } x \right) \right) - ( 1 - y ) g \left( \theta ^ { T } x \right) \right) x _ { j } \\
    & = \quad - \left( y - h ( x ) \right) x _ { j }
  \end{aligned}
\end{equation}
This therefore gives us the stochastic gradient ascent rule:
\begin{equation}
  \theta _ { j } : = \theta _ { j } + \alpha \left( y ^ { ( i ) } - h \left( x ^ { ( i ) } \right) \right) x _ { j } ^ { ( i ) }
\end{equation}
We must use gradient descent for logistic regression since there is no closed
form solution for this problem.
%------------------------------------------------
\section{Softmax Regression}
A softmax regression, also called a multiclass logistic regression, is
used to generalize logistic regression when there are more than 2 outcome
classes.
\subsection{Softmax Function}
\subsection{MLE and Cost Function}
\subsection{Gradient Descent}
%------------------------------------------------
\section{Generalized Linear Models}
\subsection{Exponentional Family}
\subsection{Assumptions of GLMs}
\subsection{Examples}
\subsubsection{Ordinary Least Squares}
\subsubsection{Logistic Regression}
\subsubsection{Softmax Regression}
%------------------------------------------------
\section{Perceptron}
%------------------------------------------------
\section{Support Vector Machines}
%------------------------------------------------
\section{Generative Learning: Gaussian Discriminant Analysis}
\subsection{Assumptions}
\subsection{Estimation}
%------------------------------------------------
\section{Generative Learning: Naive Bayes}
\subsection{Assumptions}
\subsection{Estimation}
%------------------------------------------------
\section{Tree-based Methods}
%------------------------------------------------
\section{K-Nearest Neighbors}
%------------------------------------------------
%------------------------------------------------
\section{K-Means Clustering}
\subsection{Hierarchical Clustering}
\subsection{Clustering Metrics}
%------------------------------------------------
\section{Expectation-Maximization}
\subsection{Mixture of Gaussians}
\subsection{Factor Analysis}
%------------------------------------------------
\section{Principal Component Analysis}
%------------------------------------------------
\section{Independent Component Analysis}
%------------------------------------------------
\section{Reinforcement Learning}
\subsection{Markov Decision Processes}
\subsection{Policy and Value Functions}
\subsection{Value Iteration Algorithm}
\subsection{Q-Learning}
%------------------------------------------------
\section{Hidden Markov Models}
%------------------------------------------------
\section{Deep Learning}
\subsection{Basics}
\subsection{Activation Functions}
\subsection{Loss Functions}
\subsection{Backpropagation}
\subsection{Regularization Methods}
\subsection{Optimization Algorithms}
\subsection{Convolutional Networks}
\subsection{Recurrent Networks}
\subsubsection{Elman RNN}
\subsubsection{Long Short-Term Memory}
\subsubsection{Gated Recurrent Unit}
\subsection{Autoencoders}
\subsubsection{Variational Autoencoders}
\subsection{General Adversarial Networks}
\subsection{Encoder-Decoder Models}
\subsubsection{Attention Models}

%------------------------------------------------
%------------------------------------------------
%------------------------------------------------
%------------------------------------------------
%------------------------------------------------


% References
% \bibliographystyle{abbrv}
% \bibliography{study_guide}

%------------------------------------------------

\end{document}
