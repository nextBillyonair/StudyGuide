%------------------------------------------------------------------------------
%   PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%------------------------------------------------------------------------------

\documentclass[twoside,twocolumn]{article}

%\usepackage[sc]{mathpazo} % Use the Palatino font
%\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
%\linespread{1.05} % Line spacing - Palatino needs more space between lines
%\usepackage{microtype} % Slightly tweak font spacing for aesthetics

\usepackage[english]{babel} % Language hyphenation and typographical rules

% Document margins
\usepackage[hmarginratio=1:1,top=32mm,left=20mm,right=20mm,columnsep=20pt]{geometry}
% Custom captions under/above floats in tables or figures
\usepackage[hang, small,labelfont=bf,up,textfont=it,up]{caption}
\usepackage{booktabs} % Horizontal rules in tables

\usepackage{enumitem} % Customized lists
\setlist[itemize]{noitemsep} % Make itemize lists more compact
\usepackage{textcomp}

% Allows abstract customization
\usepackage{abstract}
% Set the "Abstract" text to bold
\renewcommand{\abstractnamefont}{\normalfont\bfseries}
% Set the abstract itself to small italic text
\renewcommand{\abstracttextfont}{\normalfont\small\itshape}

\usepackage{fancyhdr} % Headers and footers
\pagestyle{fancy} % All pages have headers and footers
\fancyhead{} % Blank out the default header
\fancyfoot{} % Blank out the default footer
\fancyhead[C]{\thetitle}
\fancyfoot[RO,LE]{\thepage} % Custom footer text

\usepackage{titling} % Customizing the title section

\usepackage{hyperref} % For hyperlinks in the PDF
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{tikz}
\usetikzlibrary{bayesnet, arrows, positioning, fit, arrows.meta, shapes}

\usepackage{color}
\usepackage{caption}
\usepackage{subcaption}

\usepackage{graphicx}


\captionsetup[figure]{labelfont={bf},textfont=normalfont}

%------------------------------------------------------------------------------
%   TITLE SECTION
%------------------------------------------------------------------------------
\newlength\mystoreparindent
\newenvironment{myparindent}[1]{%
  \setlength{\mystoreparindent}{\the\parindent}
  \setlength{\parindent}{#1}
  }{%
  \setlength{\parindent}{\mystoreparindent}
}

\setlength{\droptitle}{-4\baselineskip} % Move the title up

\pretitle{\begin{center}\Huge\bfseries} % Article title formatting
\posttitle{\end{center}} % Article title closing formatting

\title{Machine Learning Study Guide}
\author{%
  \textsc{William Watson} \\[1ex]
  \normalsize Johns Hopkins University \\
  \normalsize \href{mailto:billwatson@jhu.edu}{billwatson@jhu.edu}
}

\date{}%\today} % Leave empty to omit a date
% \renewcommand{\maketitlehookd}{%

% }

%------------------------------------------------------------------------------

\begin{document}

% Print the title
\maketitle

%------------------------------------------------------------------------------
%   ARTICLE CONTENTS
%------------------------------------------------------------------------------
\tableofcontents
%------------------------------------------------
\onecolumn
%------------------------------------------------
\section{Linear Algebra and Calculus}
\subsection{General Notation}
A vector $x \in \mathbb{R}^n$ has $n$ entries, and $x_i \in \mathbb{R}$ is the
$i$-th entry:
\begin{equation}
  x=\left(\begin{array}{c}x_1\\x_2\\\vdots\\x_n\end{array}\right)\in\mathbb{R}^n
\end{equation}
We denote a matrix $A \in \mathbb{R}^{m \times n}$ with $m$ rows and $n$
columns, and $A_{ij} \in \mathbb{R}$ is the entry in the $i$-th row and
$j$-th column:
\begin{equation}
  A=\left(\begin{array}{ccc}A_{11}& \cdots&A_{1n}\\\vdots&& \vdots\\A_{m1}& \cdots&A_{mn}\end{array}\right)\in\mathbb{R}^{m\times n}
\end{equation}
Vectors can be viewed as a $n \times 1$ matrix.
\subsubsection{Indentity Matrix}
The identity matrix $I\in\mathbb{R}^{n\times n}$ is a square matrix with ones
along the diagonal and zero everywhere else:
\begin{equation}
  I=\left(\begin{array}{cccc}1&0& \cdots&0\\0& \ddots& \ddots& \vdots\\\vdots& \ddots& \ddots&0\\0& \cdots&0&1\end{array}\right)
\end{equation}
For all matrices $A \in \mathbb{R}^{n \times n}$ we have
$A \times I = I \times A = A$.
\subsubsection{Diagonal Matrix}
A diagonal matrix $D \in \mathbb{R}^{n \times n}$
is a square matrix with nonzero values along the diagonal and zero everywhere
else:
\begin{equation}
  D=\left(\begin{array}{cccc}d_1&0& \cdots&0\\0& \ddots& \ddots& \vdots\\\vdots& \ddots& \ddots&0\\0& \cdots&0&d_n\end{array}\right)
\end{equation}
The diagonal matrix $D$ is also written as $\textrm{diag}(d_1,\hdots,d_n)$.
\subsubsection{Orthogonal Matrix}
Two vectors $x, y \in \mathbb{R}^n$ are orthogonal if $x^T y = 0$. A vector
$x \in \mathbb{R}^n$ is normalized if $||x||_2 = 1$.
A square matrix $U\in \mathbb{R}^{n \times n}$ is orthogonal if all its
columns are orthogonal to each other and are normalized. Hence:
\begin{equation}
  U^TU = I = UU^T
\end{equation}
Hence the inverse of an orthogonal matrix is its transpose.m
\subsection{Matrix Operations}
\subsubsection{Vector-Vector Products}
Given two vectors $x$, $y \in \mathbb{R}^n$, the inner product is:
\begin{equation}
  x^T y = \sum_{i=1}^n x_i y_i \in \mathbb{R}
\end{equation}
The outer product for a vector $x \in \mathbb{R}^m$, $y \in \mathbb{R}^n$ is:
\begin{equation}
  xy^T=\left(\begin{array}{ccc}x_1y_1& \cdots&x_1y_n\\\vdots&\ddots& \vdots\\x_my_1& \cdots&x_my_n\end{array}\right)\in\mathbb{R}^{m\times n}
\end{equation}
\subsubsection{Vector-Matrix Products}
The product of a matrix $A \in \mathbb{R}^{m \times n}$ and vector
$x \in \mathbb{R}^n$ is a vector $y = Ax \in \mathbb{R}^m$. If we write $A$
by the rows, $Ax$ is expressed as:
\begin{equation}
  y = A x = \left( \begin{array} { c } { - \,\, a _ { 1 } ^ { T } \,\,- } \\ { - \,\,a _ { 2 } ^ { T }\,\, -} \\ { \vdots } \\ { - \,\, a _ { m } ^ { T } \,\,-} \end{array} \right) x = \left( \begin{array} { c } { a _ { 1 } ^ { T } x } \\ { a _ { 2 } ^ { T } x } \\ { \vdots } \\ { a _ { m } ^ { T } x } \end{array} \right)
\end{equation}
Here, the $i$-th entry of $y$ is the inner product of the $i$-th row of $A$ and
$x$, $y_i = a_i^T x$. If we write $A$ is column form:
\begin{equation}
  y = A x = \left( \begin{array} { c c c c } { | } & { | } & { } & { | } \\ { a _ { 1 } } & { a _ { 2 } } & { \cdots } & { a _ { n } } \\ { | } & { | } & { } & { | } \end{array} \right) \left( \begin{array} { c } { x _ { 1 } } \\ { x _ { 2 } } \\ { \vdots } \\ { x _ { n } } \end{array} \right) = a _ { 1 } x _ { 1 } + a _ { 2 } x _ { 2 } + \ldots +  a _ { n } x _ { n }
\end{equation}
Here, $y$ is a linear combination of the columns of $A$, where the coefficients
of the linear combination are given by the entries of $x$.
\subsubsection{Matrix-Matrix Products}
Given a matrix $A\in \mathbb{m \times n}$ and matrix $B \in \mathbb{n \times p}$,
we can define $C = AB$ as follows:
\begin{equation}
  C = AB = \left( \begin{array} { c c c c } { a _ { 1 } ^ { T } b _ { 1 } } & { a _ { 1 } ^ { T } b _ { 2 } } & { \cdots } & { a _ { 1 } ^ { T } b _ { p } } \\ { a _ { 2 } ^ { T } b _ { 1 } } & { a _ { 2 } ^ { T } b _ { 2 } } & { \cdots } & { a _ { 2 } ^ { T } b _ { p } } \\ { \vdots } & { \vdots } & { \ddots } & { \vdots } \\ { a _ { m } ^ { T } b _ { 1 } } & { a _ { m } ^ { T } b _ { 2 } } & { \cdots } & { a _ { m } ^ { T } b _ { p } } \end{array} \right)
\end{equation}
Hence, each $(i,j)$-th entry of $C$ is equal to the inner product of the $i$-th
row of $A$ and the $j$-th column of $B$. Compactly:
\begin{equation}
  C_{ij} = a_i^T b_j = \sum_{k=1}^n a_{ik} b_{kj}
\end{equation}
\subsubsection{The Transpose}
The transpose of a matrix $A\in \mathbb{R}^{m \times n}$ is
$A^T \in \mathbb{R}^{n \times m}$ matrix whose entries are:
\begin{equation}
  (A^T)_{ij} = A_{ji}
\end{equation}
Properties of the transpose:
\begin{enumerate}
  \item $(A^T)^T = A$
  \item $(AB)^T = B^T A^T$
  \item $(A + B)^T = A^T + B^T$
\end{enumerate}
\subsubsection{The Trace}
The trace of a square matrix $A \in \mathbb{R}^{n \times m}$ is denoted
$\operatorname{tr}(A)$. It is the sum of diagonal elements in the matrix:
\begin{equation}
  \operatorname{tr}A = \sum_{i=1}^n A_{ii}
\end{equation}
Properties of the trace:
\begin{enumerate}
  \item For $A \in \mathbb{R}^{n \times n}$, $\operatorname{tr}A=\operatorname{tr}A^T$
  \item For $A, B \in \mathbb{R}^{n \times n}$, $\operatorname{tr}(A+B)=\operatorname{tr}A+\operatorname{tr}B$
  \item For $A \in \mathbb{R}^{n \times n}$, $t\in \mathbb{R}$, $\operatorname{tr}(tA)=t \cdot \operatorname{tr}A$
  \item For $A, B$ such that $AB$ is square, $\operatorname{tr}AB = \operatorname{tr}BA$
  \item For $A,B,C$ such that $ABC$ is square, $\operatorname{tr}ABC=\operatorname{tr}BCA = \operatorname{tr}CAB$, and so on
\end{enumerate}
\subsubsection{The Inverse}
The inverse of a matrix $A$ is noted $A^{-1}$ and is the unique matrix
such that:
\begin{equation}
  A^{-1}A=I=AA^{-1}
\end{equation}
Not all square matrices are invertible. In addition, assuming
$A, B \in \mathbb{R}^{n \times n}$ are non-singular:
\begin{enumerate}
  \item $(A^{-1})^{-1} = A$
  \item $(AB)^{-1} = B^{-1}A^{-1}$
  \item $(A^{-1})^T = (A^T)^{-1}$
\end{enumerate}
\subsubsection{The Determinant}
The determinant of a square matrix $A\in\mathbb{R}^{n\times n}$, noted $|A|$
or $\textrm{det}(A)$ is expressed recursively in terms of
$A_{\backslash i, \backslash j}$, which is the matrix $A$ without
its $i$-th row and $j$-th column, as follows:
\begin{equation}
  \textrm{det}(A)=|A|=\sum_{j=1}^n(-1)^{i+j}A_{i,j}|A_{\backslash i,\backslash j}|
\end{equation}
Remark that $A$ is invertible if and only if $|A| \not= 0$. Also, $|AB|=|A||B|$
and $|A^T|=|A|$.
\subsection{Matrix Properties}
\subsubsection{Norms}
A norm of a vector $x$ is any function $f:\mathbb{R}^{n} \rightarrow \mathbb{R}$
that satisfies 4 properties:
\begin{enumerate}
  \item Non-negativity: For all $x \in \mathbb{R}^n$, $f(x) \geq 0$
  \item Definiteness: $f(x)=0$ if and only if $x=0$
  \item Homogeneity: For all $x \in \mathbb{R}^n$, $t \in \mathbb{R}$,
    $f(tx) = |t| f(x)$
  \item Triangle Inequality: For all $x, y \in \mathbb{R}^n$, $f(x+y) \leq f(x) + f(y)$
\end{enumerate}
However, most norms used come from the family of $\ell_p$ norms:
\begin{equation}
  \ell_p = ||x||_p = \left( \sum_{i=1}^n x_i^p \right)^{\frac{1}{p}}
\end{equation}
The $p$-norm is used in Holder's inequality. The Manhattan norm, used in
LASSO regularization, is:
\begin{equation}
  \ell_1 = ||x||_1 = \sum_{i=1}^n |x_i|
\end{equation}
The euclidean norm, $\ell_2$ is used in ridge regularization and
distance measures:
\begin{equation}
  \ell_2 = || x ||_2 = \sqrt{\sum_{i=1}^n x_i^2}
\end{equation}
Finally, the infinity norm is used in uniform convergence:
\begin{equation}
  \ell_{\infty} = || x ||_{\infty} = \underset{i}{\textrm{max }}|x_i|
\end{equation}
The Frobenius norm of a matrix is analogous to the $\ell_2$ norm of a vector:
\begin{equation}
  ||A||_F = \sqrt{\sum_{ij}A^2_{ij}}
\end{equation}
The dot product of two vectors can be expressed in terms of norms:
\begin{equation}
  x^T y = ||x||_2 ||y||_2 \cos \theta
\end{equation}
where $\theta$ is the angle between vectors $x$ and $y$.
\subsubsection{Linear Dependence and Rank}
A set of vectors $\{x_1, x_2, \hdots, x_n\} \subset \mathbb{R}^m$ is linearly
independent if no vector can be represented as a linear combination of the
remaining vectors. Conversely, if one vector in the set can be represented
as a linear combination of the remaining vectors, then the vectors are
said to be linearly dependent. Formally, for scalar values
$\alpha_1, \hdots, \alpha_{n-1} \in \mathbb{R}$:
\begin{equation}
  x_n = \sum_{i=1}^{n-1} \alpha_i x_i
\end{equation}
The rank of a given matrix $A$ is noted $\operatorname{rank}(A)$
and is the dimension of the vector space generated by its columns.
This is equivalent to the maximum number of linearly independent columns of
$A$. If $\operatorname{rank}(A) = \min(m,n)$, then $A$ is said to be full rank.m
\subsubsection{Span, Range, and Nullspace}
The span of a set of vectors $\{x_1, x_2, \hdots, x_n\}$ is the set of all
vectors that can be expressed as a linear combination of
$\{x_1, x_2, \hdots, x_n\}$. Formally,
\begin{equation}
  \operatorname { span } \left( \left\{ x _ { 1 } , \ldots x _ { n } \right\} \right) = \left\{ v : v = \sum _ { i = 1 } ^ { n } \alpha _ { i } x _ { i } , \quad \alpha _ { i } \in \mathbb { R } \right\}
\end{equation}
The projection of a vector $y \in \mathbb{R}^n$ onto the span of
$\{x_1, x_2, \hdots, x_n\}$ is the vector
$v \in \operatorname { span } \left( \left\{ x _ { 1 } , \ldots x _ { n } \right\} \right)$
such that $v$ is as close as possible to $y$ as measured by the euclidean norm:
\begin{equation}
  \operatorname { Proj } \left( y ; \left\{ x _ { 1 } , \ldots x _ { n } \right\} \right) = \operatorname { argmin } _ { v \in \operatorname { span } \left( \left\{ x _ { 1 } , \ldots , x _ { n } \right\} \right) } \| y - v \| _ { 2 }
\end{equation}
The range of a matrix $A\in \mathbb{R}^{m \times n}$ is the span of the columns
of $A$:
\begin{equation}
  \mathcal { R } ( A ) = \left\{ v \in \mathbb { R } ^ { m } : v = A x , x \in \mathbb { R } ^ { n } \right\}
\end{equation}
The nullspace of a matrix $A \in \mathbb{R}^{m \times n}$ is the set of all
vectors that equal $0$ when multiplied by $A$:
\begin{equation}
  \mathcal { N } ( A ) = \left\{ x \in \mathbb { R } ^ { n } : A x = 0 \right\}
\end{equation}
\subsubsection{Symmetric Matrices}
A square matrix $A \in \mathbb{R}^{n \times n}$ is symmetric if $A=A^T$. It
is anti-symmetric if $A=-A^T$. For any matrix $A\in \mathbb{R}^{n \times n}$
the matrix $A+A^T$ is symmetric and the matrix $A-A^T$ is anti-symmetric.
From this, any square matrix $A \in \mathbb{R}^{n \times n}$ can be represented
as a sum of a symmetric matrix and an anti-symmetric matrix:
\begin{equation}
  A=\underbrace{\frac{A+A^T}{2}}_{\textrm{Symmetric}}+\underbrace{\frac{A-A^T}{2}}_{\textrm{Antisymmetric}}
\end{equation}
\subsubsection{Positive Semidefinite Matrices}
Given a square matrix $A \in \mathbb{R}^{n \times n}$ and a vector
$x \in \mathbb{R}^n$, the scalar value $x^TAx$ is called the quadratic form:
\begin{equation}
  x^TAx = \sum_{i=1}^n \sum_{j=1}^n A_{ij} x_i x_j
\end{equation}
A symmetric matrix $A$ is:
\begin{enumerate}
  \item Positive definite (PD) if for all nonzero vectors, $x^TAx > 0$
  \item Positive semidefinite (PSD) if for all nonzero vectors, $x^TAx \geq 0$
  \item Negative definite (ND) if for all nonzero vectors, $x^TAx < 0$
  \item Negative semidefinite (NSD) if for all nonzero vectors, $x^TAx \leq 0$
  \item Indefinite if it is neiter PSD nor NSD, i.e. if there exists a $x_1, x_2$
    such that $x^T_1Ax_1 > 0$ and $x^T_2Ax_2 < 0$
\end{enumerate}
Given any matrix $A \in \mathbb{R}^{m \times n}$, the gram matrix $G = A^T A$
is always PSD. If $m \geq n$ and $A$ is full rank, then $G$ is PD.
\subsubsection{Eigendecomposition}
Given a square matrix $A \in \mathbb{R}^{n \times n}$, we say that
$\lambda \in \mathbb{C}$ is an eigenvalue of $A$ and $v \in \mathbb{C}$ is the
corresponding eigenvector if
\begin{equation}
  Av = \lambda v, \quad v \not= 0
\end{equation}
The trace of $A$ is equal to the sum of its eigenvalues:
\begin{equation}
  \operatorname{tr}A = \sum_{i=1}^n \lambda_i
\end{equation}
The determinant of $A$ is equal to the product of its eigenvalues:
\begin{equation}
  |A| = \prod_{i=1}^n \lambda_i
\end{equation}
Suppose matrix $A$ has $n$ linearly independent eigenvectors
$\{v_1, \hdots, v_n\}$ with corresponding eigenvalues
$\{\lambda_1, \hdots, \lambda_n\}$. Let $V$ be a matrix with one eigenvalue
per column: $V = \left[ v_1, \hdots, v_n \right]$. Let
$\Lambda =\textrm{diag}(\lambda_1,\hdots,\lambda_n)$. Then the eigendecomposition
of $A$ is given by:
\begin{equation}
  A = V \Lambda V^{-1}
\end{equation}
For when $A$ is symmetric, then the eigenvalues of $A$ are real, the
eigenvectors of $A$ are orthonormal, and hence $V$ is an orthogonal matrix
(henceforth renamed $U$). Hence $A = U \Lambda U^T$.
A matrix whose eigenvalues are all positive is called positive definite.
A matrix whose eigenvalues are all positive or zero valued is called positive
semidefinite. Likewise, if all eigenvalues are negative, the matrix is
negative definite, and if all eigenvalues are negative or zero valued,
it is negative semidefinite. In addition, assuming sorted eigenvalues,
for the following optimization problem:
\begin{equation}
  \max _ { x \in \mathbb { R } ^ { n } } x ^ { T } A x \quad \text { subject to } \| x \| _ { 2 } ^ { 2 } = 1
\end{equation}
The solution is $v_1$ the eigenvector corresponding to $\lambda_1$. For
the minimization problem:
\begin{equation}
  \min _ { x \in \mathbb { R } ^ { n } } x ^ { T } A x \quad \text { subject to } \| x \| _ { 2 } ^ { 2 } = 1
\end{equation}
the optimal solution for $x$ is $v_n$, the eigenvector corresponding to
eigenvalue $\lambda_n$.
\subsubsection{Singlular Value Decomposition}
The singular value decomposition provides a way to factorize a $m \times n$
matrix $A$ into singular vectors and singular values. It is defined as:
\begin{equation}
  A = U D V^T
\end{equation}
Suppose $A \in \mathbb{R}^{m \times n}$ matrix. Then
$U \in \mathbb{R}^{m \times m}$, $D \in \mathbb{R}^{m \times n}$,
$ V \in \mathbb{R}^{n \times n}$ matrices. The matricies $U$ and $V$ are
orthogonal, and matrix $D$ is diagonal. The elements along the diagonal of $D$
are known as the singular values of matrix $A$. The columns of $U$ are known
as the left-singular vectors while the columns of $V$ are the right-singular
vectors. The left-singular vectors of $A$ are the eigenvectors of $AA^T$.
The right-singular vectors of $A$ are the eigenvectors of $A^TA$. We can use
SVD to partially generalize matrix inversion to nonsquare matrices.
\subsubsection{The Moore-Penrose Pseudoinverse}
Matrix inversion is not defined for matrices that are not square.
Note that for nonsingular $A$:
\begin{equation}
  A A^{-1}A = A
\end{equation}
However, if the inverse is not defined, we seek to find a matrix $A^{+}$ such
that:
\begin{equation}
  A A^{+}A = A
\end{equation}
The moore-penrose pseudoinverse $A^+$ is defined as follows:
\begin{equation}
  A^+ = \lim_{\alpha \to 0} (A^TA + \alpha I)^{-1}A^T
\end{equation}
Practical algorithms use the singular value decomposition of $A$ such that:
\begin{equation}
  A^+ = V D^+ U^T
\end{equation}
where $U, D, V$ are from the SVD of $A$, and the pseudoinverse of $D^+$ is
obtained by taking the reciprocal of the nonzero diagonal elements of $D$.
If $A$ has more columns than rows, then using the pseudoinverse to solve a
linear equation $Ax=y$ provides one of many solutions, but provides $x = A^+ y$
with minimal euclidean norm $||x||_2$. When $A$ has more rows than columns,
the pseudoinverse gives us the $x$ for which $Ax$ is as close as possible
to $y$, i.e. minimizing $||Ax-y||_2$.
\subsection{Matrix Calculus}
\subsubsection{The Gradient}
Let $f:\mathbb{R}^{m\times n}\rightarrow\mathbb{R}$ be a function and
$A \in \mathbb{R}^{m \times n}$ be a matrix. The gradient of $f$ with respect to
$A$ is a $m \times n$ matrix noted as $\nabla_A f(A)$ such that:
\begin{equation}
  \nabla _ { A } f ( A ) \in \mathbb { R } ^ { m \times n } = \left(
  \begin{array} { c c c c } { \frac { \partial f ( A ) } { \partial A _ { 11 } } } & { \frac { \partial f ( A ) } { \partial A _ { 12 } } } & { \cdots } & { \frac { \partial f ( A ) } { \partial A _ { 1 } } } \\ { \frac { \partial f ( A ) } { \partial A _ { 21 } } } & { \frac { \partial f ( A ) } { \partial A _ { 22 } } } & { \cdots } & { \frac { \partial f ( A ) } { \partial A _ { 2 n } } } \\ { \vdots } & { \vdots } & { \ddots } & { \vdots } \\ { \frac { \partial f ( A ) } { \partial A _ { m 1 } } } & { \frac { \partial f ( A ) } { \partial A _ { m 2 } } } & { \cdots } & { \frac { \partial f ( A ) } { \partial A _ { m n } } } \end{array} \right)
\end{equation}
Or compactly for each $ij$ entry:
\begin{equation}
  \nabla _ { A } f ( A ) _ { i j } = \frac { \partial f ( A ) } { \partial A _ { i j } }
\end{equation}
However, the gradient of a vector $x \in \mathbb{R}^n$ is:
\begin{equation}
  \nabla _ { x } f ( x ) = \left( \begin{array} { c } { \frac { \partial f ( x ) } { \partial x _ { 1 } } } \\ { \frac { \partial f ( x ) } { \partial x _ { 2 } } } \\ { \vdots } \\ { \frac { \partial f ( x ) } { \partial x _ { n } } } \end{array} \right)
\end{equation}
\subsubsection{The Hessian}
Let $f:\mathbb{R}^{n}\rightarrow\mathbb{R}$ be a function and
$x \in \mathbb{R}^{n}$ be a vector. The hessian of $f$ with respect to
$x$ is a $n \times n$ symmetric matrix noted as $H = \nabla_x^2 f(x)$ such that:
\begin{equation}
  \nabla _ { x } ^ { 2 } f ( x ) \in \mathbb { R } ^ { n \times n } = \left( \begin{array} { c c c c } { \frac { \partial ^ { 2 } f ( x ) } { \partial x _ { 1 } ^ { 2 } } } & { \frac { \partial ^ { 2 } f ( x ) } { \partial x _ { 2 } } } & { \cdots } & { \frac { \partial ^ { 2 } f ( x ) } { \partial x _ { 1 } \partial x _ { n } } } \\ { \frac { \partial ^ { 2 } f ( x ) } { \partial x _ { 2 } \partial x _ { 1 } } } & { \frac { \partial ^ { 2 } f ( x ) } { \partial x _ { 2 } ^ { 2 } } } & { \cdots } & { \frac { \partial ^ { 2 } f ( x ) } { \partial x _ { 2 } \partial x _ { n } } } \\ { \vdots } & { \vdots } & { \ddots } & { \vdots } \\ { \frac { \partial ^ { 2 } f ( x ) } { \partial x _ { n } \partial x _ { 1 } } } & { \frac { \partial ^ { 2 } f ( x ) } { \partial x _ { n } \partial x _ { 2 } } } & { \cdots } & { \frac { \partial ^ { 2 } f ( x ) } { \partial x _ { n } ^ { 2 } } } \end{array} \right)
\end{equation}
Or compactly:
\begin{equation}
  \nabla _ { x } ^ { 2 } f ( x ) _ { i j } = \frac { \partial ^ { 2 } f ( x ) } { \partial x _ { i } \partial x _ { j } }
\end{equation}
Note that the hessian is only defined when $f(x)$ is real-valued.
\subsubsection{Gradient Properties}
For matrices $A, B, C$ and vectors $x, b$:
\begin{enumerate}
  \item $\nabla_x b^Tx = b$
  \item $\nabla_x x^TAx = 2Ax$ (if $A$ symmetric)
  \item $\nabla^2_x x^TAx = 2A$ (if $A$ symmetric)
  \item $\nabla_A\textrm{tr}(AB)=B^T$
  \item $\nabla_{A^T}f(A)=\left(\nabla_Af(A)\right)^T$
  \item $\nabla_A\textrm{tr}(ABA^TC)=CAB+C^TAB^T$
  \item $\nabla_A|A|=|A|(A^{-1})^T$
\end{enumerate}
%------------------------------------------------
\section{Convex Optimization}
\subsection{Convexity}
\subsubsection{Convex Sets}
A set $C$ is convex if, for any $x,y \in C$ and $\alpha \in \mathbb{R}$ with
$0 \leq \alpha \leq 1$, we have
\begin{equation}
  \alpha x + (1-\alpha) y \in C
\end{equation}
This point is known as a convex combination of $x$ and $y$.
\subsubsection{Convex Functions}
A function
$f: \mathbb{R}^n \to \mathbb{R}$ is convex if its domain $\mathcal{D}(f)$ is
a convex set, and if, for all $x, y \in \mathcal{D}(f)$ and
$\alpha \in \mathbb{R}$, $0 \leq \alpha \leq 1$, we have:
\begin{equation}
  f(\alpha x + (1-\alpha) y) \leq \alpha f(x) + (1-\alpha) f(y)
\end{equation}
A function is strictly convex if:
\begin{equation}
  f(\alpha x + (1-\alpha) y) < \alpha f(x) + (1-\alpha) f(y)
\end{equation}
\subsubsection{First-Order Conditions}
Suppose a function $f: \mathbb{R}^n \to \mathbb{R}$ is differentiable, then
$f$ is convex if and only if $\mathcal{D}(f)$ is a convex set and for all
$x, y \in \mathcal{D}(f)$ we have:
\begin{equation}
  f(y) \geq f(x) + \nabla_xf(x)^T(y-x)
\end{equation}
This is called the first-order approximation to the function $f$ at point $x$.
This is approximating $f$ with its tangent line at point $x$.
The first order condition for convexity says that $f$ is convex if and only if
the tangent line is a global underestimator of the function $f$.
In other words, if we take our function and draw a tangent line at any point,
then every point on this line will lie below the corresponding point on $f$.
\subsubsection{Second-Order Conditions}
Suppose a function $f: \mathbb{R}^n \to \mathbb{R}$ is twice differentiable,
i.e. the hessian $\nabla^2_xf(x)$ is defined for all points $x$ in the domain
of $f$. Then $f$ is convex if and only if $\mathcal{D}(f)$ is a convex set and
its hessian is PSD:
\begin{equation}
  \nabla^2_xf(y) \succeq 0
\end{equation}
\subsubsection{Jensen's Inequality}
Suppose we start with the inequality in the basic definition of a convex function:
\begin{equation}
  f(\alpha x + (1-\alpha) y) \leq \alpha f(x) + (1-\alpha) f(y)
\end{equation}
We can extend this to convex combinations of more than one point:
\begin{equation}
  f\left(\sum_{i=1}^k \alpha_i x_i\right) \leq \sum^k_{i=1} \alpha_i f(x_i)
\end{equation}
For $\sum_k \alpha = 1$ and all $\alpha \geq 0$. This can be extended to
integrals:
\begin{equation}
  f \left( \int p ( x ) x d x \right) \leq \int p ( x ) f ( x ) dx
\end{equation}
For $\int p(x)dx = 1$ and $p(x) \geq 0 \forall x$. This implies $p(x)$ is a
probability density, and we can write our inequality in terms of expectations:
\begin{equation}
  f ( \mathbf { E } [ x ] ) \leq \mathbf { E } [ f ( x ) ]
\end{equation}
And is known as Jensen's inequality.
\subsubsection{Sublevel Sets}
Given a convex function $f:\mathbb{R}^n \to \mathbb{R}$ and a real number
$\alpha \in \mathbb{R}$, the $\alpha$-sublevel set is:
\begin{equation}
  \{x \in \mathcal{D}(f): f(x) \leq \alpha\}
\end{equation}
In other words, the $\alpha$-sublevel set is the set of all points $x$
such that $f(x) \leq \alpha$.
\subsection{Convex Optimization}
A convex optimization problem seeks to optimize a convex function $f$ with
respect to a variable $x$ and possible constraints.
\begin{equation}
  \begin{array} { l l } { \text { minimize } } & { f ( x ) } \\ { \text { subject to } } & { g _ { i } ( x ) \leq 0 , \quad i = 1 , \ldots , m } \\ { } & { h _ { i } ( x ) = 0 , \quad i = 1 , \ldots , p } \end{array}
\end{equation}
where $f$ is a convex function, $g_i$ are convex functions, and $h_i$ are affine
functions. The optimal value $p^*$ is equal to the minimum possible value of
the objective function in the feasible region:
\begin{equation}
  p ^ { \star } = \min \left\{ f ( x ) : g _ { i } ( x ) \leq 0 , i = 1 , \ldots , m , h _ { i } ( x ) = 0 , i = 1 , \ldots , p \right\}
\end{equation}
The optimal point $x^*$ is a point such that $f(x^*) = p^*$.
\subsubsection{Global Optimality}
A point $x$ is locally optimal if it is feasible and if there exists some
$R > 0$ such that all feasible points $z$ with $||x-z||_2 \leq R$, satisfy
$f(x) \leq f(z)$. This means that for $x$ to be locally optimal, there
exists no nerby points that have a lower objective value. A point $x$ is globally
optimal if it is feadible and for all feasible points $z$, $f(x) \leq f(z)$.
For a convex optimization problem, all locally optimal points are globally
optimal.
\subsubsection{Gradient Descent}
Using $\alpha \in \mathbb{R}$ as the learning rate, we can update a set of
parameters $\theta$ with respect to minimizing a function $f$ as follows:
\begin{equation}
  \theta := \theta-\alpha\nabla f(\theta)
\end{equation}
Stochastic gradient descent (SGD) is updating the parameter based on each
training example, and batch gradient descent is on a batch of training
examples.
\subsubsection{Newton's Algorithm}
Newton's algorithm is a numerical method using information from the second
derivative to find $\theta$ such that $f'(\theta)=0$.
\begin{equation}
  \theta := \theta-\frac{f'(\theta)}{f''(\theta)}
\end{equation}
For multidimensional parameters:
\begin{equation}
  \theta := \theta-\alpha H^{-1} \nabla_\theta f(\theta)
\end{equation}
Where $H$ is the hessian matrix of second partial derivatives.
\begin{equation}
  H_{ij} = \frac { \partial ^ { 2 } f ( \theta ) } { \partial \theta _ { i } \partial \theta _ { j } }
\end{equation}
\subsection{Lagrange Duality and KKT Conditions}
Generic differentiable convex optimization problems are of the form:
\begin{equation}
  \begin{array} { l l } { \text { minimize } } & { f ( x ) } \\ { \text { subject to } } & { g _ { i } ( x ) \leq 0 , \quad i = 1 , \ldots , m } \\ { } & { h _ { i } ( x ) = 0 , \quad i = 1 , \ldots , p } \end{array}
\end{equation}
where $x\in \mathbb{R}^n$ is the optimization variable,
$f: \mathbb{R}^n \to \mathbb{R}$ and $g_i: \mathbb{R}^n \to \mathbb{R}$
are differentiable convex functions, and $h_i: \mathbb{R}^n \to \mathbb{R}$
are affine functions.
\subsubsection{The Lagrangian}
Given a convex constrained minimization problem, the generalized lagrangian
is a function $\mathcal{L}: \mathbb{R}^n \times \mathbb{R}^m \times \mathbb{R}^p
\to \mathbb{R}$ defined as:
\begin{equation}
  \mathcal { L } ( x , \alpha , \beta ) = f ( x ) + \sum _ { i = 1 } ^ { m } \alpha _ { i } g _ { i } ( x ) + \sum _ { i = 1 } ^ { p } \beta _ { i } h _ { i } ( x )
\end{equation}
We refer to $x$ as the primal variables of the Lagrangian. The $\alpha$ and
$\beta$ are refered to as the dual variables or lagrange multipliers. The
key idea behind Lagrangian duality is for any convex optimization problem,
there always exist settings of the dual variables such that the unconstrained
minimum of the Lagrangian with respect to the primal variables (keeping the
dual variables fixed) coincides with the solution of the original constrained
minimization problem.
\subsubsection{Primal and Dual Problems}
The primal problem is:
\begin{equation}
  \min _ { x } \underbrace { \left[ \max _ { \alpha , \beta : \alpha _ { i } \geq 0 , \forall i } \mathcal { L } ( x , \alpha , \beta ) \right] } _ { \theta _ { \mathcal { P } ( x ) } } = \min _ { x } \theta _ { \mathcal { P } } ( x )
\end{equation}
Here, $\theta_{\mathcal{P}} : \mathbb{R}^n \to \mathbb{R}$ is the primal objective, and
the right hand side is the primal problem. In addition,  $p^* = \theta_{\mathcal{P}}(x^*)$
is the optimal value of the primal objective. \\
The dual problem is:
\begin{equation}
  \max _ { \alpha , \beta : \alpha _ { i } \geq 0 , \forall i  } \underbrace { \left[ \min _ { x } \mathcal { L } ( x , \alpha , \beta ) \right] } _ { \theta _ { \mathcal { D } ( x ) } } = \max _ { \alpha , \beta : \alpha _ { i } \geq 0 , \forall i  } \theta _ { \mathcal { D } } ( x )
\end{equation}
Here, $\theta_{\mathcal{D}} : \mathbb{R}^m \times \mathbb{R}^p \to \mathbb{R}$
is the dual objective, and
the right hand side is the dual problem. In addition,
$s^* = \theta_{\mathcal{D}}(\alpha^*, \beta^*)$
is the optimal value of the dual objective.
\subsubsection{Strong and Weak Duality}
Weak duality for any pair of primal and dual problems, with $d^*$ the optimal
objective value of the dual problem and $p^*$ as the optimal objective value
of the primal problem we have:
\begin{equation}
  d^* \leq p^*
\end{equation}
Strong duality is when for any pair of primal and dual problems which satisfy
certain technical conditions called constraint qualifications, then:
\begin{equation}
  d^* = p^*
\end{equation}
\subsubsection{Complementary Slackness}
If strong duality holds, then $\alpha^*_i g(x^*_i) = 0$ for each $i=1,\hdots,m$.
This property is known as complementary slackness. This implies the following:
\begin{equation}
  \alpha _ { i } ^ { * } > 0 \Longrightarrow g _ { i } \left( x ^ { * } \right) = 0
\end{equation}
\begin{equation}
  g _ { i } \left( x ^ { * } \right) < 0 \quad \Longrightarrow \quad \alpha _ { i } ^ { * } = 0
\end{equation}
\subsubsection{The KKT Conditions}
Suppose that $x^* \in \mathbb{R}^n$, $\alpha^* \in \mathbb{R}^m$ and
$\beta^* \in \mathcal{R}^p$ satisfy the following conditions:
\begin{enumerate}
  \item Primal feasibility: $g_i(x^*) \leq 0$, for $i=1,\hdots, m$ and
    $h_i(x^*)=0$ for $i=1,\hdots, p$
  \item Dual feasibility: $\alpha^*_i \geq 0$ for $i=1,\hdots, m$
  \item Complementary slackness: $\alpha^*_i g_i(x^*) = 0$ for $i=1,\hdots, m$
  \item Lagrangian stationarity: $\nabla_x \mathcal{L}(x^*, \alpha^*, \beta^*) = \vec{0}$
\end{enumerate}
Then $x^*$ is primal optimal and $(\alpha^*, \beta^*)$ are dual optimal.
Furthermore, if strong duality holds, then any primal optimal $x^*$ and dual
optimal $(\alpha^*, \beta^*)$ must satisfy the conditions $1$ through $4$.
These conditions are known as the Karush-Kuhn-Tucker (KKT) conditions.
%------------------------------------------------
\section{Probability and Statistics}
\subsection{Basics}
The set of all possible outcomes of an experiment is known as the sample space
and denoted by $S$. Any subset $E$ of the sample space is known as an event.
An event is a set consisting of possible outcomes of the experiment.
\subsubsection{Axioms of Probability}
For each event $E$, we denote $P(E)$ as the probability of event $E$ occuring.
$P(E)$ satisfies the following properties:
\begin{enumerate}
  \item Every probability is between 0 and 1 included:
    \begin{equation} 0 \leq P(E) \leq 1 \end{equation}
  \item The probability that at least one event in the sample space will occur
  is $1$: \begin{equation} P(S) = 1 \end{equation}
  \item For any sequence of mutually exclusive events $E_1,\hdots,E_n$ we have:
    \begin{equation} P\left(\bigcup_{i=1}^nE_i\right)=\sum_{i=1}^nP(E_i) \end{equation}
\end{enumerate}
\subsubsection{Permutation}
A permutation is an arrangement of $r$ objects from a pool of $n$ objects, in
a given order. The number of such arrangements is given by $P(n, r)$:
\begin{equation} P(n, r) = \frac{n!}{(n-r)!} \end{equation}
\subsubsection{Combination}
A combination is an arrangement of $r$ objects from a pool of $n$ objects, where
order does not matter. The number of such arrangements is given by $C(n, r)$:
\begin{equation} C(n, r) = \frac{P(n, r)}{r!} = \frac{n!}{r! (n-r)!} \end{equation}
Note that for $0 \leq r \leq n$ we have $P(n, r) \geq C(n, r)$.
\subsection{Conditional Probability}
Let $B$ be an event with non-zero probability, the conditional proability of
any event $A$ given $B$ is:
\begin{equation} P(A | B) = \frac{P(A \cap B)}{P(B)} \end{equation}
\subsubsection{Bayes Rule}
For events $A, B$ such that $P(B) > 0$, we have:
\begin{equation} P(A | B) = \frac{P(B|A) P(A)}{P(B)} \end{equation}
From Bayes rule, we have:
\begin{equation} P(A \cap B) = P(A|B)P(B) = P(A)P(B|A) \end{equation}
Let $\{ A_i, i \in [1, n] \}$ be such that for all $i$, $A_i \not= \varnothing$.
We say that $\{A_i\}$ is a partition if we have:
\begin{equation} \forall i\neq j, A_i\cap A_j=\emptyset\quad\mbox{ and }\quad\bigcup_{i=1}^nA_i=S \end{equation}
Remark that for any event $B$ in the sample space, we have:
\begin{equation} P(B)=\sum_{i=1}^n P(B|A_i)P(A_i) \end{equation}
Let $\{ A_i, i \in [1, n] \}$ be a partition of the sample space, we can
extend bayes rule as:
\begin{equation} P(A_k|B)=\frac{P(B|A_k)P(A_k)}{\displaystyle\sum_{i=1}^nP(B|A_i)P(A_i)} \end{equation}
\subsubsection{Independence}
Two events $A$ and $B$ are independent if and only if we have:
\begin{equation} P(A \cap B) = P(A) P(B) \end{equation}
\subsection{Random Variables}
A random variable $X$ is a function that maps every element in a sample space
to a real line.
\subsubsection{Cumulative Distribution Function (CDF)}
The cumulative distribution function $F$, which is monotonically non-decreasing
and is such that $\lim_{x \to -\infty}F(X) = 0$ and $\lim_{x \to \infty}F(X) = 1$
is defined as:
\begin{equation} F(x) = P(X \leq x) \end{equation}
In addition:
\begin{equation} P(a < X \leq b) = F(b) - F(a) \end{equation}
\subsubsection{Probability Density Function (PDF)}
The probability density function is the derivative of the CDF.
It has the following properties:
\begin{enumerate}
  \item $f(x) \geq 0$
  \item $\int^{\infty}_{-\infty} f(x) = 1$
  \item $\int_{x \in A} f(x) dx = P(X \in A)$
\end{enumerate}
\subsubsection{Discrete PDF/CDF}
If $X$ is discrete, by denoting $f$ as the PDF and $F$ as the CDF, we have:
\begin{equation}
  F(X) = \sum_{x_i \leq x} P(X = x_i)
\end{equation}
\begin{equation}
  f(x_j) = P(X = x_j)
\end{equation}
And the following properties for the PDF:
\begin{equation}
  0 \leq f(x_j) \leq 1
\end{equation}
\begin{equation}
  \sum_j f(x_j) = 1
\end{equation}
\subsubsection{Continuous PDF/CDF}
If $X$ is continuous, by denoting $f$ as the PDF and $F$ as the CDF, we have:
\begin{equation}
  F(X) = \int_{-\infty}^x f(y) dy
\end{equation}
\begin{equation}
  f(x) = \frac{dF}{dx}
\end{equation}
And the following properties for the PDF:
\begin{equation}
  f(x) \geq 0
\end{equation}
\begin{equation}
  \int_{-\infty}^{\infty} f(x) dx = 1
\end{equation}
\subsubsection{Expectation}
The expected value of a random variable, also known as the mean value or
first moment, is denoted as $E[X]$ or $\mu$. It is the value obtained by
averaging the results of a random variable.
We use $\mbox{(D)}$ for discrete, $\mbox{(C)}$ for continuous.
\begin{equation}
  \mbox{(D)}\quad E[X]=\sum_{i=1}^nx_if(x_i)\quad\quad\mbox{and}\quad\mbox{(C)}\quad E[X]=\int_{-\infty}^{+\infty}xf(x)dx
\end{equation}
The expected value of a function of a random variable $g(X)$ is:
\begin{equation}
  \mbox{(D)}\quad E[g(X)]=\sum_{i=1}^ng(x_i)f(x_i)\quad\quad\mbox{and}\quad\mbox{(C)}\quad E[g(X)]=\int_{-\infty}^{+\infty}g(x)f(x)dx
\end{equation}
The $k$-th moment, noted $E[X^k]$ is the value of $X^k$ that we expect to observe
on average on infinitely many trials. The $k$-th moment is a case of the previous
definition with $g \,:\, X \mapsto X^k$.
\begin{equation}
  \mbox{(D)}\quad E[X^k]=\sum_{i=1}^nx_i^kf(x_i) \quad\quad\mbox{and}\quad\mbox{(C)}\quad E[X^k]=\int_{-\infty}^{+\infty}x^kf(x)dx
\end{equation}
A characteristic function $\psi(\omega)$ is derived from a probability density
function $f(x)$ and is defined as:
\begin{equation}
  \mbox{(D)}\quad \psi(\omega)=\sum_{i=1}^nf(x_i)e^{i\omega x_i} \quad\quad\mbox{and}\quad\mbox{(C)}\quad \psi(\omega)=\int_{-\infty}^{+\infty}f(x)e^{i\omega x}dx
\end{equation}
Remark that $e^{i\omega x} = \cos(\omega x)+ i \sin(\omega x)$. The $k$-th moment
can also be computed with the characteristic function as:
\begin{equation}
  E[X^k]=\frac{1}{i^k}\left[\frac{\partial^k\psi}{\partial\omega^k}\right]_{\omega=0}
\end{equation}
\subsubsection{Variance and Standard Deviation}
The variance of a random variable, often noted $\mbox{Var}(X)$ or $\sigma^2$,
is a measure of the spread of its distribution function.
It is determined as follows:
\begin{equation}
  \mbox{Var}[X]=E[(X-E[X])^2]=E[X^2]-E[X]^2
\end{equation}
The standard deviation of a random variable, often noted $\sigma$, is a
measure of the spread of its distribution function which is
compatible with the units of the actual random variable.
It is determined as follows:
\begin{equation}
  \sigma = \sqrt{\mbox{Var}[X]}
\end{equation}
Note that the variance for any constant $a$ is $\mbox{Var}[a]= 0$, and
$\mbox{Var}[af(X)]=a^2\mbox{Var}[f(X)]$.
\subsection{Discrete Random Variables}
\subsubsection{Bernoulli}
For $X\sim Bernoulli(p)$, we define a binary event with a probability of $p$
for a true event, and a false event with probability of $q=1-p$.
\begin{equation}
  P(X=x) = \begin{cases} q=1-p & \textrm{if } x=0 \\ p & \textrm{if } x=1 \end{cases}
\end{equation}
It can also be expressed as:
\begin{equation}
  f(x; p) = p^k (1-p)^{1-k} \quad \mbox{for } k\in \{0, 1\}
\end{equation}
Other properties:
\begin{equation}
  E[X] = p
\end{equation}
\begin{equation}
  \mbox{Var}[X] = pq = p (1-p)
\end{equation}
\begin{equation}
  \psi(\omega) = (1-p) + p e^{i\omega}
\end{equation}
\subsubsection{Binomial}
For $X\sim Binomial(n, p)$, the number of true events in $n$ independent
experiments, with true probability of $p$, false probability of $q=1-p$.
\begin{equation}
  \displaystyle P(X=x)=\displaystyle\binom{n}{x} p^x(1-p)^{n-x}
\end{equation}
Other properties:
\begin{equation}
  E[X] = np
\end{equation}
\begin{equation}
  \mbox{Var}[X] = npq = np(1-p)
\end{equation}
\begin{equation}
  \psi(\omega) = (pe^{i\omega}+(1-p))^n
\end{equation}
\subsubsection{Geometric}
For $X\sim Geometric(p)$, is the number of experiments with true
probability of $p$ until the first true event (number of trials to get one success).
\begin{equation}
  \displaystyle P(X=x)=p(1-p)^{x-1}
\end{equation}
Other properties:
\begin{equation}
  E[X] = \frac{1}{p}
\end{equation}
\begin{equation}
  \mbox{Var}[X] = \frac{1-p}{p^2}
\end{equation}
\begin{equation}
  \psi(\omega) = \frac{pe^{i \omega}}{1-(1-p)e^{i \omega}}
\end{equation}
\subsubsection{Poisson}
For $X\sim Poisson(\lambda)$, for $\lambda > 0$, a probability distribution
over the nonnegative integers
used for modeling the frequency of rare events.
\begin{equation}
  \displaystyle P(X=x)=\frac{\lambda^x}{x!}e^{-\lambda}
\end{equation}
Other properties:
\begin{equation}
  E[X] = \lambda
\end{equation}
\begin{equation}
  \mbox{Var}[X] = \lambda
\end{equation}
\begin{equation}
  \psi(\omega) = e^{\lambda(e^{i\omega}-1)}
\end{equation}
\subsection{Continuous Random Variables}
\subsubsection{Uniform}
For $X\sim Uniform(a,b)$, we have equal probability density to every value
between $a$ and $b$.
\begin{equation}
  f(x) = \begin{cases} \frac{1}{b-a} & \textrm{if } a \leq x \leq b \\ 0 & \textrm{otherwise} \end{cases}
\end{equation}
Other properties:
\begin{equation}
  E[X] = \frac{a+b}{2}
\end{equation}
\begin{equation}
  \mbox{Var}[X] = \frac{(b-a)^2}{12}
\end{equation}
\begin{equation}
  \psi(\omega) = \displaystyle\frac{e^{i\omega b}-e^{i\omega a}}{(b-a)i\omega}
\end{equation}
\subsubsection{Exponential}
For $X\sim Exponential(\lambda)$, $\lambda > 0$, is the
decaying probability density over the nonnegative reals.
\begin{equation}
  f(x) = \begin{cases} \lambda e^{-\lambda x} & \textrm{if } x \geq 0 \\ 0 & \textrm{otherwise} \end{cases}
\end{equation}
Other properties:
\begin{equation}
  E[X] = \frac{1}{\lambda}
\end{equation}
\begin{equation}
  \mbox{Var}[X] = \frac{1}{\lambda^2}
\end{equation}
\begin{equation}
  \psi(\omega) = \displaystyle\frac{1}{1-\frac{i\omega}{\lambda}}
\end{equation}
\subsubsection{Gaussian (Normal)}
For $X\sim Normal(\mu, \sigma)$, denoted also $X \sim \mathcal{N}(\mu, \sigma)$.
\begin{equation}
  \displaystyle f(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2}
\end{equation}
Other properties:
\begin{equation}
  E[X] = \mu
\end{equation}
\begin{equation}
  \mbox{Var}[X] = \sigma^2
\end{equation}
\begin{equation}
  \psi(\omega) = e^{i\omega\mu-\frac{1}{2}\omega^2\sigma^2}
\end{equation}
\subsection{Jointly Distributed Random Variables}
\subsubsection{Marginal Density}
\subsubsection{Cumulative Distribution}
\subsubsection{Conditional Density}
\subsubsection{Independence}
\subsubsection{Expectation and Covariance}
\subsubsection{Correlation}
\subsection{Parameter Estimation}
\subsubsection{Definitions}
\subsubsection{Bias}
\subsubsection{Mean and Central Limit Theorem}
\subsubsection{Variance}
\subsection{Probability Bounds and Inequalities}
\subsubsection{Markov}
\subsubsection{Chebyshev}
\subsubsection{Chernoff}
\subsubsection{Hoeffding}


\section{Information Theory}
Information Theory revolves around quantifying how much information is present
in a signal. The basic intuition lies in the fact that learning an unlikely
event has occured is more informative than learning that a likely event has
occured. The basics are:
\begin{enumerate}
  \item Likely events should have low information content, and in the extreme
  case, events that are guaranteed to happen should have no information content
  whatsoever.
  \item Less likely events should have higher information content.
  \item Independent events should have additive information.
\end{enumerate}
We satisfy all three properties by defining self-information of an event $x$
for a probability distribution $P$ as:
\begin{equation}
  I(x) = -\log P(x)
\end{equation}
We can quantify the amount of uncertainty in a
distribution using Shannon entropy:
\begin{equation}
  H ( P ) = \mathbb { E } _ { \mathrm { x } \sim P } [ I ( x ) ] = - \mathbb { E } _ { \mathrm { x } \sim P } [ \log P ( x ) ]
\end{equation}
Which in the discrete setting is written as:
\begin{equation}
  H ( P ) = -\sum_{x} P(x) \log P(x)
\end{equation}
In other words, the Shannon entropy of a distribution is the expected amount
of information in an event drawn from that distribution. It gives a lower bound
on the number of bits needed on average to encode symbols drawn
from a distribution $P$. If we have two separate probability distributions
$P(x)$ and $Q(x)$ over the same random variable $\mathrm{x}$, we can measure how
different these two distributions are using the Kullback-Leibler (KL)
divergence:
\begin{equation}
  \begin{split}
    D _ { \mathrm { KL } } ( P \| Q ) \quad =& \quad \mathbb { E } _ { \mathbf { x } \sim P } \left[ \log \frac { P ( x ) } { Q ( x ) } \right]\\
    \\
    =& \quad \mathbb { E } _ { \mathbf { x } \sim P } \left[ \log P ( x ) - \log Q ( x ) \right] \\
    \\
    =& \quad \sum_x P(x) \frac{\log P(x)}{\log Q(x)} \\
  \end{split}
\end{equation}
In the case of discrete variables, it is the extra amount of information
needed to send a message containing symbols drawn from probability distribution
$P$, when we use a code that was designed to minimize the length of messages
drawn from probability distribution $Q$. The KL divergence is always
non-negative, and is $0$ if and only if $P$ and $Q$ are the same. We can
relate the KL divergence to cross-entropy.
\begin{equation}
  \begin{split}
    H(P, Q) \quad =& \quad H(P) + D _ { \mathrm { KL } } ( P \| Q ) \\
    \\
    =& \quad - \mathbb { E } _ { \mathbf { x } \sim P } \left[ \log Q ( x ) \right]\\
    \\
    =& \quad -\sum_x P(x) \log Q(x) \\
  \end{split}
\end{equation}
Minimizing the cross-entropy with respect to $Q$ is equivalent to
minimizing the KL divergence, because $Q$ does not participate in the
omitted term (entropy is constant).
%------------------------------------------------
\section{Learning Theory}
\subsection{Bias and Variance}
\subsection{Notation}
\subsubsection{Union Bound}
\subsubsection{Hoeffding Inequality}
\subsection{Training Error}
\subsection{Probably Approximately Correct (PAC)}
\subsection{Hypothesis Classes}
\subsubsection{Shattering}
\subsubsection{Upper Bound Theorem}
\subsubsection{VC Dimension}
\subsubsection{Vapnik Theorem}
%------------------------------------------------
\section{Linear Regression}
Linear Regression seeks to approximate a real valued label $y$ as a
linear function of $x$:
\begin{equation}
  h_{\theta}(x) = \theta_0 + \theta_1 \cdot x_1 + \cdots + \theta_n \cdot x_n
\end{equation}
The $\theta_i$'s are the parameters, or weights. If we include the
intercept term via $x_0=1$, we can write our model more compactly as:
\begin{equation}
  h(x) = \sum_{i=0}^{n} \theta_i \cdot x_i = \theta^T x
\end{equation}
Here $n$ is the number of input variables, or features. In Linear Regression,
we seek to make $h(x)$ as close to $y$ for a set of training examples. We
define the cost function as:
\begin{equation}
  J(\theta) = \frac{1}{2} \sum_{i=1}^{m} \left( h \left(x^{(i)}\right) - y^{(i)}\right)^2
\end{equation}
\subsection{LMS Algorithm}
We seek to find a set of $\theta$ such that we minimize $J(\theta)$ via a
search algorithm that starts at some initial guess for our parameters and takes
incremental steps to make $J(\theta)$ smaller until convergence. This is know
as gradient descent:
\begin{equation}
  \theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j}J(\theta)
\end{equation}
Here, $\alpha$ is the learning rate. We can derive the partial derivative as:
\begin{equation}
  \begin{split}
    \frac{\partial}{\partial \theta_{j}}J(\theta) \quad =& \quad \frac{\partial}{\partial \theta_{j}}\frac{1}{2} \left( h(x)-y \right)^{2} \\
    =& \quad 2\cdot \frac{1}{2} \left(h(x) - y \right) \cdot \frac{\partial}{\partial \theta_{j}} (h(x) - y) \\
    =& \quad \left(h(x) - y \right) \cdot \frac{\partial}{\partial \theta_{j}} \left(\sum_{i=0}^{n} \theta_{i} x_{i} - y \right) \\
    =& \quad \left( h ( x ) - y \right) x _ { j } \\
\end{split}
\end{equation}
Hence, for a single example (stochastic gradient descent):
\begin{equation}
  \theta _ { j } : = \theta _ { j } + \alpha \left( y ^ { ( i ) } - h \left( x ^ { ( i ) } \right) \right) x _ { j } ^ { ( i ) }
\end{equation}
This is called the LMS update rule. For a batched version, we can evaluate the
gradient on a set of examples (batch gradient descent), or the full set
(gradient descent).
\begin{equation}
  \theta _ { j } : = \theta _ { j } + \alpha \sum _ { i = 1 } ^ { m } \left( y ^ { ( i ) } - h \left( x ^ { ( i ) } \right) \right) x _ { j } ^ { ( i ) }
\end{equation}
\subsection{The Normal Equations}
We can also directly minimize $J$ without using an iterative algorithm. We
define $X$ as the matrix of all samples of size $m$ by $n$.
We let $\vec{y}$ be a $m$ dimensional vector of all target values. We can
define our cost function $J$ as:
\begin{equation}
  J(\theta) = \frac { 1 } { 2 } ( X \theta - \vec { y } ) ^ { T } ( X \theta - \vec { y } ) = \frac { 1 } { 2 } \sum _ { i = 1 } ^ { m } \left( h \left( x ^ { ( i ) } \right) - y ^ { ( i ) } \right) ^ { 2 }
\end{equation}
We then take the derivative and find its roots.

\begin{equation}
  \begin{split}
    \nabla _ { \theta } J ( \theta ) \quad =& \quad \nabla _ { \theta } \frac { 1 } { 2 } ( X \theta - \vec { y } ) ^ { T } ( X \theta - \vec { y } ) \\
    =& \quad \frac { 1 } { 2 } \nabla _ { \theta } \left( \theta ^ { T } X ^ { T } X \theta - \theta ^ { T } X ^ { T } \vec { y } - \vec { y } ^ { T } X \theta + \vec { y } ^ { T } \vec { y } \right) \\
    =& \quad \frac { 1 } { 2 } \nabla _ { \theta } \left( \operatorname { tr } \theta ^ { T } X ^ { T } X \theta - 2 \operatorname { tr } \vec { y } ^ { T } X \theta \right) \\
    =& \quad \frac { 1 } { 2 } \left( X ^ { T } X \theta + X ^ { T } X \theta - 2 X ^ { T } \vec { y } \right) \\
    =& \quad X ^ { T } X \theta - X ^ { T } \vec { y } \\
  \end{split}
\end{equation}
To minimize $J$, we set its derivatives to zero, and obtain the normal
equations:
\begin{equation}
  X ^ { T } X \theta = X ^ { T } \vec { y }
\end{equation}
Which solves $\theta$ for a value that minimizes $J(\theta)$ in closed form:
\begin{equation}
  \theta = \left( X ^ { T } X \right) ^ { - 1 } X ^ { T } \vec { y }
\end{equation}
\subsection{Probabilistic Interpretation}
Why does linear regression use the least-squares cost function? Assume that
the target variables and inputs are related via:
\begin{equation}
  y ^ { ( i ) } = \theta ^ { T } x ^ { ( i ) } + \epsilon ^ { ( i ) }
\end{equation}
Here, $\epsilon^{(i)}$ is an error term for noise. We assume each
$\epsilon^{(i)}$ is independently and identically distributed according to a
Gaussian distribution with mean zero and some variance $\sigma^2$. Hence,
$\epsilon^{(i)} \sim \mathcal { N } \left( 0 , \sigma ^ { 2 } \right)$, so
the density for any sample $x^{(i)}$ with label $y^{(i)}$ is
$y^{(i)} | x^{(i)}; \theta \sim \mathcal{N} \left( \theta^T x^{(i)}, \sigma^{2} \right)$.
This implies:
\begin{equation}
  p \left( y ^ { ( i ) } | x ^ { ( i ) } ; \theta \right) = \frac { 1 } { \sqrt { 2 \pi } \sigma } \exp \left( - \frac { \left( y ^ { ( i ) } - \theta ^ { T } x ^ { ( i ) } \right) ^ { 2 } } { 2 \sigma ^ { 2 } } \right)
\end{equation}
The probability of a dataset $X$ is quantified by a likelihood function:
\begin{equation}
  L ( \theta ) = L ( \theta ; X , \vec { y } ) = p ( \vec { y } | X ; \theta )
\end{equation}
Since we assume independence on each noise term (and samples), we can write
the likelihood function as:
\begin{equation}
  \begin{aligned}
    L ( \theta ) & = \prod _ { i = 1 } ^ { m } p \left( y ^ { ( i ) } | x ^ { ( i ) } ; \theta \right) \\
    & = \prod _ { i = 1 } ^ { m } \frac { 1 } { \sqrt { 2 \pi } \sigma } \exp \left( - \frac { \left( y ^ { ( i ) } - \theta ^ { T } x ^ { ( i ) } \right) ^ { 2 } } { 2 \sigma ^ { 2 } } \right)
  \end{aligned}
\end{equation}
To get the best choice of parameters $\theta$, we perform maximum likelihood
estimation such that $L(\theta)$ is maximized. Usually we take the negative log
and minimize:
\begin{equation}
  \begin{aligned}
    \ell ( \theta ) & = - \log L ( \theta ) \\
    & = - \log \prod _ { i = 1 } ^ { m } \frac { 1 } { \sqrt { 2 \pi } \sigma } \exp \left( - \frac { \left( y ^ { ( i ) } - \theta ^ { T } x ^ { ( i ) } \right) ^ { 2 } } { 2 \sigma ^ { 2 } } \right) \\
    & = - \sum _ { i = 1 } ^ { m } \log \frac { 1 } { \sqrt { 2 \pi } \sigma } \exp \left( - \frac { \left( y ^ { ( i ) } - \theta ^ { T } x ^ { ( i ) } \right) ^ { 2 } } { 2 \sigma ^ { 2 } } \right) \\
    & = - m \log \frac { 1 } { \sqrt { 2 \pi } \sigma } + \frac { 1 } { \sigma ^ { 2 } } \cdot \frac { 1 } { 2 } \sum _ { i = 1 } ^ { m } \left( y ^ { ( i ) } - \theta ^ { T } x ^ { ( i ) } \right) ^ { 2 }
  \end{aligned}
\end{equation}
Hence, maximizing $L(\theta$) is the same as minimizing the negative log
likelihood $\ell(\theta)$, which for linear regression is the least squares
cost function:
\begin{equation}
  \frac { 1 } { 2 } \sum _ { i = 1 } ^ { m } \left( y ^ { ( i ) } - \theta ^ { T } x ^ { ( i ) } \right) ^ { 2 }
\end{equation}
Under the previous probabilistic assumptions on the data, least-squares
regression corresponds to finding the maximum likelihood estimate of $\theta$.
This is thus one set of assumptions under which least-squares regression
can be justified as performing maximum likelihood estimation. Note that $\theta$
is independent of $\sigma^2$.
\subsection{Locally Weighted Linear Regression}
Locally Weighted Regression, also known as LWR, is a variant of
linear regression that weights each training example in its cost function by
$w^{(i)}(x)$, which is defined with parameter $\tau \in \mathbb{R}$ as:
\begin{equation}
  w^{(i)}(x)=\exp\left(-\frac{(x^{(i)}-x)^2}{2\tau^2}\right)
\end{equation}
Hence, in LWR, we do the following:
\begin{enumerate}
  \item Fit $\theta$ to minimize $\sum_i w^{(i)} \left( y^{(i)} - \theta^{T} x^{(i)} \right)^{2}$
  \item Output $\theta^T x$
\end{enumerate}
This is a non-parametric algorithm, where non-parametric refers to the fact
that the amount of information we need to represent the hypothesis $h$ grows
linearly with the size of the training set.
%------------------------------------------------
\section{Logistic Regression}
We can extend this learning to classification problems, where we have binary
labels $y$ that are either $0$ or $1$.
\subsection{The Logistic Function}
For logistic regression, our new hypothesis for estimating the class of a
sample $x$ is:
\begin{equation}
  h( x ) = g \left( \theta ^ { T } x \right) = \frac { 1 } { 1 + e ^ { - \theta ^ { T } x } }
\end{equation}
where $g(z)$ is the logistic or sigmoid function:
\begin{equation}
  g ( z ) = \frac { 1 } { 1 + e ^ { - z } }
\end{equation}
The sigmoid function is bounded between $0$ and $1$, and tends towards $1$ as
$z \rightarrow \infty$. It tends towards $0$ when $z \rightarrow -\infty$.
A useful property of the sigmoid function is the form of its derivative:
\begin{equation}
  \begin{aligned} g ^ { \prime } ( z ) \quad & = \quad \frac { d } { d z } \frac { 1 } { 1 + e ^ { - z } } \\
    & = \quad \frac { 1 } { \left( 1 + e ^ { - z } \right) ^ { 2 } } \left( e ^ { - z } \right) \\
    & = \quad \frac { 1 } { \left( 1 + e ^ { - z } \right) } \cdot \left( 1 - \frac { 1 } { \left( 1 + e ^ { - z } \right) } \right) \\
    & = \quad g ( z ) ( 1 - g ( z ) )
  \end{aligned}
\end{equation}
\subsection{Cost Function}
To fit $\theta$ for a set of training examples, we assume that:
\begin{equation}
  \begin{aligned}
    P ( y = 1 | x ; \theta ) & = h( x ) \\
    P ( y = 0 | x ; \theta ) & = 1 - h( x )
  \end{aligned}
\end{equation}
This can be written more compactly as:
\begin{equation}
  p ( y | x ; \theta ) = \left( h ( x ) \right) ^ { y } \left( 1 - h ( x ) \right) ^ { 1 - y }
\end{equation}
Assume $m$ training examples generated independently, we define the likelihood
function of the parameters as:
\begin{equation}
  \begin{aligned}
    L ( \theta ) \quad & = \quad p \left( \vec{y} | X ; \theta \right) \\
    & = \quad \prod _ { i = 1 } ^ { m } p \left( y ^ { ( i ) } | x ^ { ( i ) } ; \theta \right) \\
    & = \quad \prod _ { i = 1 } ^ { m } \left( h \left( x ^ { ( i ) } \right) \right) ^ { y ^ { ( i ) } } \left( 1 - h  \left( x ^ { ( i ) } \right) \right) ^ { 1 - y ^ { ( i ) } }
  \end{aligned}
\end{equation}
And taking the negative log likelihood to minimize:
\begin{equation}
  \begin{aligned}
    \ell ( \theta ) \quad & = \quad -\log L ( \theta ) \\
    & = \quad -\sum _ { i = 1 } ^ { m } y ^ { ( i ) } \log h \left( x ^ { ( i ) } \right) + \left( 1 - y ^ { ( i ) } \right) \log \left( 1 - h \left( x ^ { ( i ) } \right) \right)
  \end{aligned}
\end{equation}
This is known as the binary cross-entropy loss function.
\subsection{Gradient Descent}
Lets start by working with just one training example (x,y), and take
derivatives to derive the stochastic gradient ascent rule:
\begin{equation}
  \begin{aligned}
    \frac { \partial } { \partial \theta _ { j } } \ell ( \theta ) \quad & = \quad - \left( y \frac { 1 } { g \left( \theta ^ { T } x \right) } - ( 1 - y ) \frac { 1 } { 1 - g \left( \theta ^ { T } x \right) } \right) \frac { \partial } { \partial \theta _ { j } } g \left( \theta ^ { T } x \right) \\
    & = \quad - \left( y \frac { 1 } { g \left( \theta ^ { T } x \right) } - ( 1 - y ) \frac { 1 } { 1 - g \left( \theta ^ { T } x \right) } \right) g \left( \theta ^ { T } x \right) \left( 1 - g \left( \theta ^ { T } x \right) \right) \frac { \partial } { \partial \theta _ { j } } \theta ^ { T } x \\
    & = \quad - \left( y \left( 1 - g \left( \theta ^ { T } x \right) \right) - ( 1 - y ) g \left( \theta ^ { T } x \right) \right) x _ { j } \\
    & = \quad - \left( y - h ( x ) \right) x _ { j }
  \end{aligned}
\end{equation}
This therefore gives us the stochastic gradient ascent rule:
\begin{equation}
  \theta _ { j } : = \theta _ { j } + \alpha \left( y ^ { ( i ) } - h \left( x ^ { ( i ) } \right) \right) x _ { j } ^ { ( i ) }
\end{equation}
We must use gradient descent for logistic regression since there is no closed
form solution for this problem.
%------------------------------------------------
\section{Softmax Regression}
A softmax regression, also called a multiclass logistic regression, is
used to generalize logistic regression when there are more than 2 outcome
classes.
\subsection{Softmax Function}
The softmax function creates a probability distribution over a set of $k$
classes for a training example $x$, with $\theta_k$ denoting the set of
parameters to be optimzed for the $k$-th class.
\begin{equation}
  p(y=k | x; \theta) = \frac{\exp \left(\theta_k^T x \right)}{\sum_j \exp \left( \theta_j^T x \right)}
\end{equation}
\subsection{MLE and Cost Function}
We can write the maximum likelihood function for softmax regression as:
\begin{equation}
  L(\theta) = \prod_{i=1}^m \prod_k p(y=k|x; \theta)^{\mathbf{1}\{y_i=k\}}
\end{equation}
Where $\mathbf{1} \{y_i=k\}$ is the indicator function which is $1$ if its
argument is true, $0$ otherwise.
By taking the negative log likelihood:
\begin{equation}
  \begin{aligned}
    \ell ( \theta ) \quad & = \quad -\log L ( \theta ) \\
    & = \quad - \log \prod_{i=1}^m \prod_k p(y=k|x; \theta)^{\mathbf{1}\{y_i=k\}} \\
    & = \quad -\sum_{i=1}^m \sum_k \left( \mathbf{1}\{y_i=k\} \cdot \left( \theta^T_k x_i - \log \left( \sum_j \exp \left( \theta_j^T x_i \right) \right) \right) \right) \\
    & = \quad \sum_{i=1}^m -\theta^T_{y_i} x_i + \log \left( \sum_j \exp \left( \theta_j^T x_i \right) \right) \\
  \end{aligned}
\end{equation}
This is known as the cross-entropy loss function.
\subsection{Gradient Descent}
To perform gradient descent, we must take the derivative of our cost function,
but it is important to note that the derivative for the correct class is
different than the other classes.
\begin{equation}
  \begin{aligned}
    \nabla_{\theta_j} \ell(\theta) \quad & = \quad \nabla_{\theta_j} \left( \sum_{i=1}^m -\theta^T_{y_i} x_i + \log \left( \sum_k \exp \left( \theta_k^T x_i \right) \right) \right) \\
      & = \quad \sum_{i=1}^m \nabla_{\theta_j} \left( -\theta^T_{y_i} x_i \right)  +  \nabla_{\theta_j} \left( \log \left( \sum_k \exp \left( \theta_k^T x_i \right) \right) \right) \\
      & = \quad \sum_{i=1}^m \mathbf{1}\{y_i=j\} \cdot (-x_i) + \frac{\exp(\theta_j^T x_i)}{\sum_k \exp(\theta_k^T x_i)} \cdot x_i \\
      & = \quad \sum_{i=1}^m \left(\frac{\exp(\theta_j^T x_i)}{\sum_k \exp(\theta_k^T x_i)} -  \mathbf{1}\{y_i=j\} \right) \cdot x_i \\
  \end{aligned}
\end{equation}
And our update equation for the $j$-th parameter weights is:
\begin{equation}
  \theta_{j} : = \theta_{j} - \alpha \left( \frac{\exp(\theta_j^T x_i)}{\sum_k \exp(\theta_k^T x_i)} -  \mathbf{1}\{y_i=j\} \right) \cdot x_i
\end{equation}
Note that since each class has a set of weights, our gradient is a matrix known
as the jacobian $\mathbf{J}$, with $k$ classes each with $n$ feature weights.
\begin{equation}
  \mathbf{J}_{\theta} =
    \left[
      \begin{array} { c c c } {\frac { \partial \ell(\theta)  } { \partial \theta _ { 1 } } } & { \cdots } & { \frac { \partial \ell(\theta) } { \partial \theta _ { k } } } \end{array}
    \right] = \left[
      \begin{array} { c c c } {\frac { \partial \ell(\theta) } { \partial \theta _ { 11 } } } & { \cdots } & { \frac { \partial \ell(\theta) } { \partial \theta _ { k1} } } \\ { \vdots } & { \ddots } & { \vdots } \\ { \frac { \partial \ell(\theta) } { \partial \theta _ { 1n } } } & { \cdots } & { \frac { \partial \ell(\theta) } { \partial \theta _ { kn } } } \end{array}
    \right]
\end{equation}
%------------------------------------------------
\section{Generalized Linear Models}
\subsection{Exponentional Family}
\subsection{Assumptions of GLMs}
\subsection{Examples}
\subsubsection{Ordinary Least Squares}
\subsubsection{Logistic Regression}
\subsubsection{Softmax Regression}
%------------------------------------------------
\section{Perceptron}
%------------------------------------------------
\section{Support Vector Machines}
%------------------------------------------------
\section{Margin Classification}
%------------------------------------------------
\section{Generative Learning: Gaussian Discriminant Analysis}
\subsection{Assumptions}
\subsection{Estimation}
%------------------------------------------------
\section{Generative Learning: Naive Bayes}
\subsection{Assumptions}
\subsection{Estimation}
%------------------------------------------------
\section{Tree-based Methods}
%------------------------------------------------
\section{K-Nearest Neighbors}
The $k$-nearest neighbors algorithm, commonly known as
$k$-NN, is a non-parametric approach where the response of a data point
is determined by the nature of its $k$ neighbors from the training set.
It can be used in both classification and regression settings.
The higher the parameter $k$, the higher the bias, and the lower the parameter
$k$, the higher the variance.
\subsection{Classification}
In $k$-NN classification, the output is a class membership. An object is
classified by a majority vote of its neighbors, with the object being assigned
to the class most common among its $k$ nearest neighbors ($k$ is a positive integer,
typically small). If $k = 1$, then the object is simply assigned to the class
of that single nearest neighbor.
\subsection{Regression}
In $k$-NN regression, the output is the property value for the object.
This value is the average of the values of its k nearest neighbors.
\begin{equation} y = \frac{1}{k} \sum_{x_i \in \mathcal{N}_k(x)} y_i \end{equation}
where $\mathcal{N}_k(x)$ is the $k$ nearest points around $x$.
%------------------------------------------------
\section{K-Means Clustering}
CLustering seeks to group similiar points of data together in a cluster. We
denote $c^{(i)}$ as the cluster for data point $i$ and $\mu_j$ as the center
for cluster $j$. We denote $k$ as the number of clusters and $n$ as the
dimension of our data.
\subsection{Algorithm}
After randomly initializing the cluster centroids
$\mu_1, \mu_2, \hdots, \mu_k \in \mathbb{R}^n$, repeat until convergence:
\begin{enumerate}
  \item For every data point $i$:
    \begin{equation}
      c^{(i)}=\underset{j}{\textrm{arg min}}||x^{(i)}-\mu_j||^2
    \end{equation}
  \item For each cluster $j$:
    \begin{equation}
      \mu_j=\frac{\displaystyle\sum_{i=1}^m1_{\{c^{(i)}=j\}}x^{(i)}}{\displaystyle\sum_{i=1}^m1_{\{c^{(i)}=j\}}}
    \end{equation}
\end{enumerate}
The first step is known as cluster assignment, and the second updates the
cluster center (i.e. the average of all points in the cluster). In order to
see if it converges, use the distortion function:
\begin{equation}
  J(c,\mu)=\sum_{i=1}^m||x^{(i)}-\mu_{c^{(i)}}||^2
\end{equation}
The distortion function $J$ is non-convex, and coordinate descent of $J$ is not
guaranteed to converge to the global minimum (i.e. susceptible to local optima).
\subsection{Hierarchical Clustering}
Hierarchical clustering is a clustering algorithm with an agglomerative
hierarchical approach that builds nested clusters in a successive manner. The
types are:
\begin{enumerate}
  \item Ward Linkage: minimize within cluster distance
  \item Average Linkage: minimize average distance between cluster pairs
  \item Complete Linkage: minimize maximum distance between cluster pairs
\end{enumerate}
\subsection{Clustering Metrics}
In an unsupervised learning setting, it is often hard to assess the
performance of a model since we don't have the ground truth labels as was
the case in the supervised learning setting.
\paragraph{Silhouette coefficient} By noting $a$ and $b$ the
mean distance between a sample and all other points in the same class,
and between a sample and all other points in the next nearest cluster,
the silhouette coefficient $s$ for a single sample is defined as follows:
\begin{equation}
  s = \frac{b - a}{\max(a, b)}
\end{equation}
\paragraph{Calinskli-Harabaz Index} By noting $k$ the number of clusters, $B_k$
and $W_k$ the between and within-clustering dispersion matricies defined as:
\begin{equation}
  B_k=\sum_{j=1}^kn_{c^{(i)}}(\mu_{c^{(i)}}-\mu)(\mu_{c^{(i)}}-\mu)^T
\end{equation}
\begin{equation}
  W_k=\sum_{i=1}^m(x^{(i)}-\mu_{c^{(i)}})(x^{(i)}-\mu_{c^{(i)}})^T
\end{equation}
the Calinksli-Harabaz index $s(k)$ indicated how well a clustering model
defines its clusters, such that higher scores indicate more dense and well
separated cluster assignments. It is defined as:
\begin{equation}
  s(k)=\frac{\textrm{Tr}(B_k)}{\textrm{Tr}(W_k)}\times\frac{N-k}{k-1}
\end{equation}
%------------------------------------------------
\section{Expectation-Maximization}
\subsection{Mixture of Gaussians}
\subsection{Factor Analysis}
%------------------------------------------------
\section{Principal Component Analysis}
Principal Component Analysis is a dimension reduction technique that finds
the variance maximizing the directions onto which to project the data.
\subsection{Eigenvalues, Eigenvectors, and the Spectral Theorem}
Recall that for a given matrix $A\in \mathbb{R}^{n \times n}$, $\lambda$ is said
to be an eigenvalue of $A$ if there exists a vector
$z \in \mathbb{R}^n \backslash \{ 0 \}$, called an eigenvector, such that:
\begin{equation}
  Az=\lambda z
\end{equation}
The spectral theorem states that given matrix $A\in \mathbb{R}^{n \times n}$, if
$A$ is symmetric then $A$ is diagonalizable by a real orthogonal matrix
$U\in \mathbb{R}^{n \times n}$. Note
$\Lambda = \textrm{diag}(\lambda_1, \hdots, \lambda_n)$. Then $\exists \, \Lambda$
such that:
\begin{equation}
  A=U\Lambda U^T
\end{equation}
Note that the eigenvector associated with the largest eigenvalue is called the
principal eigenvector of matrix $A$.
\subsection{Algorithm}
The PCA procedure projects the data onto $k$ dimensions by maximizing the variance
of the data as follows:
\begin{enumerate}
  \item Normalize the data to have mean $0$, standard deviation $1$:
    \begin{equation} x^{(i)}\leftarrow\frac{x^{(i)}-\mu}{\sigma} \end{equation}
      where $\mu$ and $\sigma^2$ are:
      \begin{equation} \mu = \frac{1}{m}\sum_{i=1}^mx^{(i)} \end{equation}
      \begin{equation} \sigma^2=\frac{1}{m}\sum_{i=1}^m(x^{(i)}-\mu)^2 \end{equation}
  \item Compute covariance matrix $\Sigma$, which is symmetric with real eigenvalues.
    \begin{equation} \Sigma = \frac{1}{m}\sum_{i=1}^m x^{(i)}x^{(i)^T} \in \mathbb{R}^{n \times n} \end{equation}
  \item Compute $u_1, \hdots, u_k \in \mathbb{R}^{n}$ the $k$ orthogonal principal
    eigenvectors of $\Sigma$, i.e. the orthogonal eigenvectors of the $k$
    largest eigenvalues.
  \item Project the data on $\textrm{span}_\mathbb{R}(u_1,...,u_k)$ to create
    a vector $y^{(i)}$ from point $x^{(i)}$:
    \begin{equation} y^{(i)} = U^T x^{(i)} = \left( \begin{array} { c } { u _ { 1 } ^ { T } x ^ { ( i ) } } \\ { u _ { 2 } ^ { T } x ^ { ( i ) } } \\ { \vdots } \\ { u _ { k } ^ { T } x ^ { ( i ) } } \end{array} \right) \in \mathbb { R } ^ { k } \end{equation}
\end{enumerate}
This procedure maximizes the variance among all $k$-dimensional spaces.
%------------------------------------------------
\section{Independent Component Analysis}
%------------------------------------------------
\section{Reinforcement Learning}
\subsection{Markov Decision Processes}
\subsection{Policy and Value Functions}
\subsection{Value Iteration Algorithm}
\subsection{Q-Learning}
%------------------------------------------------
\section{Hidden Markov Models}
%------------------------------------------------
\section{Deep Learning: Basics}
\subsection{Basics}
\subsection{Activation Functions}
\subsection{Loss Functions}
\subsection{Backpropagation}
\subsection{Regularization Methods}
\subsection{Optimization Algorithms}
\subsection{Convolutional Networks}
\subsection{Recurrent Networks}
\subsubsection{Elman RNN}
\subsubsection{Long Short-Term Memory}
\subsubsection{Gated Recurrent Unit}
\subsubsection{Bidirectional RNNs}
\section{Deep Learning: Advanced}
\subsection{Autoencoders}
\subsubsection{Variational Autoencoders}
\subsection{General Adversarial Networks}
\subsection{Encoder-Decoder Models}
\subsubsection{Attention Models}

%------------------------------------------------
%------------------------------------------------
%------------------------------------------------
%------------------------------------------------
%------------------------------------------------


% References
% \bibliographystyle{abbrv}
% \bibliography{study_guide}

%------------------------------------------------

\end{document}
