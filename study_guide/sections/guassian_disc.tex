\section{Generative Learning: Gaussian Discriminant Analysis}
A generative model learns who the data is generated by estimating $P(x|y)$
which is then used to estimate $P(y|x)$ via Bayes rule.
\subsection{Assumptions}
Gaussian Discriminant Analysis assumes the following:
\begin{enumerate}
  \item $y\sim\textrm{Bernoulli}(\phi)$
  \item $x|y=0\sim\mathcal{N}(\mu_0,\Sigma)$
  \item $x|y=1\sim\mathcal{N}(\mu_1,\Sigma)$
\end{enumerate}
Writing out the distributions be have:
\begin{equation}
  P(y) = \phi^y \cdot (1-\phi)^{1-y}
\end{equation}
\begin{equation}
  P(x|y=k) = \frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}} \exp \left(-\frac{1}{2} (x-\mu_k)^T \Sigma^{-1} (x - \mu_k)\right)
\end{equation}
\subsection{Estimation}
The log-likelihood of the data is given by:
\begin{equation}
  \begin{aligned}
    \ell(\phi, \mu_0, \mu_1, \Sigma) \quad =& \quad \log \prod_{i=1}^{m}P(x^{(i)}, y^{(i)}; \phi, \mu_0, \mu_1, \Sigma) \\
    =& \quad \log \prod_{i=1}^m P(x^{(i)}| y^{(i)}; \phi, \mu_0, \mu_1, \Sigma) \cdot P(y^{(i)}; \phi)
  \end{aligned}
\end{equation}
By maximizing $\ell$ with respect to the parameters, we get the following MLE
of the parameters:
\begin{equation}
  \phi = \displaystyle\frac{1}{m}\sum_{i=1}^m1_{\{y^{(i)}=1\}}
\end{equation}
\begin{equation}
  \mu_k = \displaystyle\frac{\sum_{i=1}^m1_{\{y^{(i)}=k\}}x^{(i)}}{\sum_{i=1}^m1_{\{y^{(i)}=k\}}}
\end{equation}
\begin{equation}
  \Sigma = \displaystyle\frac{1}{m}\sum_{i=1}^m(x^{(i)}-\mu_{y^{(i)}})(x^{(i)}-\mu_{y^{(i)}})^T
\end{equation}
\subsection{Prediction}
To classify a point $x$, we find the class $y=k$ which maximizes the probability:
\begin{equation}
  \hat{y} = \underset{k}{\textrm{arg max}} \,\, P(y=k) \cdot\prod_j P(x_j | y=k)
\end{equation}
