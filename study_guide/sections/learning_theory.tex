\section{Learning Theory}
\subsection{Bias and Variance}
\subsection{Notation}
\subsubsection{Union Bound}
\subsubsection{Hoeffding Inequality For Bernoulli Variables}
\subsection{Training Error}
For a given classifier $h$, we define the training error $\widehat{\epsilon}(h)$,
also known as the empirical risk or empirical error, to be:
\begin{equation} \widehat{\epsilon}(h) = \frac{1}{m}\sum_{i=1}^m 1_{\{h(x^{(i)})\neq y^{(i)}\}} \end{equation}
\subsection{Probably Approximately Correct (PAC)}
PAC learning is a framework with the following set of assumptions:
% \begin{enumerate}
% \end{enumerate}
\subsection{Hypothesis Classes}
\subsubsection{Shattering}
\subsubsection{Upper Bound Theorem}
\subsubsection{VC Dimension}
\subsubsection{Vapnik Theorem}
