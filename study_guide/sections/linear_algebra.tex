\section{Linear Algebra and Calculus}
\subsection{General Notation}
A vector $x \in \mathbb{R}^n$ has $n$ entries, and $x_i \in \mathbb{R}$ is the
$i$-th entry:
\begin{equation}
  x=\left(\begin{array}{c}x_1\\x_2\\\vdots\\x_n\end{array}\right)\in\mathbb{R}^n
\end{equation}
We denote a matrix $A \in \mathbb{R}^{m \times n}$ with $m$ rows and $n$
columns, and $A_{ij} \in \mathbb{R}$ is the entry in the $i$-th row and
$j$-th column:
\begin{equation}
  A=\left(\begin{array}{ccc}A_{11}& \cdots&A_{1n}\\\vdots&& \vdots\\A_{m1}& \cdots&A_{mn}\end{array}\right)\in\mathbb{R}^{m\times n}
\end{equation}
Vectors can be viewed as a $n \times 1$ matrix.
\subsubsection{Indentity Matrix}
The identity matrix $I\in\mathbb{R}^{n\times n}$ is a square matrix with ones
along the diagonal and zero everywhere else:
\begin{equation}
  I=\left(\begin{array}{cccc}1&0& \cdots&0\\0& \ddots& \ddots& \vdots\\\vdots& \ddots& \ddots&0\\0& \cdots&0&1\end{array}\right)
\end{equation}
For all matrices $A \in \mathbb{R}^{n \times n}$ we have
$A \times I = I \times A = A$.
\subsubsection{Diagonal Matrix}
A diagonal matrix $D \in \mathbb{R}^{n \times n}$
is a square matrix with nonzero values along the diagonal and zero everywhere
else:
\begin{equation}
  D=\left(\begin{array}{cccc}d_1&0& \cdots&0\\0& \ddots& \ddots& \vdots\\\vdots& \ddots& \ddots&0\\0& \cdots&0&d_n\end{array}\right)
\end{equation}
The diagonal matrix $D$ is also written as $\textrm{diag}(d_1,\hdots,d_n)$.
\subsubsection{Orthogonal Matrix}
Two vectors $x, y \in \mathbb{R}^n$ are orthogonal if $x^T y = 0$. A vector
$x \in \mathbb{R}^n$ is normalized if $||x||_2 = 1$.
A square matrix $U\in \mathbb{R}^{n \times n}$ is orthogonal if all its
columns are orthogonal to each other and are normalized. Hence:
\begin{equation}
  U^TU = I = UU^T
\end{equation}
Hence the inverse of an orthogonal matrix is its transpose.m
\subsection{Matrix Operations}
\subsubsection{Vector-Vector Products}
Given two vectors $x$, $y \in \mathbb{R}^n$, the inner product is:
\begin{equation}
  x^T y = \sum_{i=1}^n x_i y_i \in \mathbb{R}
\end{equation}
The outer product for a vector $x \in \mathbb{R}^m$, $y \in \mathbb{R}^n$ is:
\begin{equation}
  xy^T=\left(\begin{array}{ccc}x_1y_1& \cdots&x_1y_n\\\vdots&\ddots& \vdots\\x_my_1& \cdots&x_my_n\end{array}\right)\in\mathbb{R}^{m\times n}
\end{equation}
\subsubsection{Vector-Matrix Products}
The product of a matrix $A \in \mathbb{R}^{m \times n}$ and vector
$x \in \mathbb{R}^n$ is a vector $y = Ax \in \mathbb{R}^m$. If we write $A$
by the rows, $Ax$ is expressed as:
\begin{equation}
  y = A x = \left( \begin{array} { c } { - \,\, a _ { 1 } ^ { T } \,\,- } \\ { - \,\,a _ { 2 } ^ { T }\,\, -} \\ { \vdots } \\ { - \,\, a _ { m } ^ { T } \,\,-} \end{array} \right) x = \left( \begin{array} { c } { a _ { 1 } ^ { T } x } \\ { a _ { 2 } ^ { T } x } \\ { \vdots } \\ { a _ { m } ^ { T } x } \end{array} \right)
\end{equation}
Here, the $i$-th entry of $y$ is the inner product of the $i$-th row of $A$ and
$x$, $y_i = a_i^T x$. If we write $A$ is column form:
\begin{equation}
  y = A x = \left( \begin{array} { c c c c } { | } & { | } & { } & { | } \\ { a _ { 1 } } & { a _ { 2 } } & { \cdots } & { a _ { n } } \\ { | } & { | } & { } & { | } \end{array} \right) \left( \begin{array} { c } { x _ { 1 } } \\ { x _ { 2 } } \\ { \vdots } \\ { x _ { n } } \end{array} \right) = a _ { 1 } x _ { 1 } + a _ { 2 } x _ { 2 } + \ldots +  a _ { n } x _ { n }
\end{equation}
Here, $y$ is a linear combination of the columns of $A$, where the coefficients
of the linear combination are given by the entries of $x$.
\subsubsection{Matrix-Matrix Products}
Given a matrix $A\in \mathbb{m \times n}$ and matrix $B \in \mathbb{n \times p}$,
we can define $C = AB$ as follows:
\begin{equation}
  C = AB = \left( \begin{array} { c c c c } { a _ { 1 } ^ { T } b _ { 1 } } & { a _ { 1 } ^ { T } b _ { 2 } } & { \cdots } & { a _ { 1 } ^ { T } b _ { p } } \\ { a _ { 2 } ^ { T } b _ { 1 } } & { a _ { 2 } ^ { T } b _ { 2 } } & { \cdots } & { a _ { 2 } ^ { T } b _ { p } } \\ { \vdots } & { \vdots } & { \ddots } & { \vdots } \\ { a _ { m } ^ { T } b _ { 1 } } & { a _ { m } ^ { T } b _ { 2 } } & { \cdots } & { a _ { m } ^ { T } b _ { p } } \end{array} \right)
\end{equation}
Hence, each $(i,j)$-th entry of $C$ is equal to the inner product of the $i$-th
row of $A$ and the $j$-th column of $B$. Compactly:
\begin{equation}
  C_{ij} = a_i^T b_j = \sum_{k=1}^n a_{ik} b_{kj}
\end{equation}
\subsubsection{The Transpose}
The transpose of a matrix $A\in \mathbb{R}^{m \times n}$ is
$A^T \in \mathbb{R}^{n \times m}$ matrix whose entries are:
\begin{equation}
  (A^T)_{ij} = A_{ji}
\end{equation}
Properties of the transpose:
\begin{enumerate}
  \item $(A^T)^T = A$
  \item $(AB)^T = B^T A^T$
  \item $(A + B)^T = A^T + B^T$
\end{enumerate}
\subsubsection{The Trace}
The trace of a square matrix $A \in \mathbb{R}^{n \times m}$ is denoted
$\operatorname{tr}(A)$. It is the sum of diagonal elements in the matrix:
\begin{equation}
  \operatorname{tr}A = \sum_{i=1}^n A_{ii}
\end{equation}
Properties of the trace:
\begin{enumerate}
  \item For $A \in \mathbb{R}^{n \times n}$, $\operatorname{tr}A=\operatorname{tr}A^T$
  \item For $A, B \in \mathbb{R}^{n \times n}$, $\operatorname{tr}(A+B)=\operatorname{tr}A+\operatorname{tr}B$
  \item For $A \in \mathbb{R}^{n \times n}$, $t\in \mathbb{R}$, $\operatorname{tr}(tA)=t \cdot \operatorname{tr}A$
  \item For $A, B$ such that $AB$ is square, $\operatorname{tr}AB = \operatorname{tr}BA$
  \item For $A,B,C$ such that $ABC$ is square, $\operatorname{tr}ABC=\operatorname{tr}BCA = \operatorname{tr}CAB$, and so on
\end{enumerate}
\subsubsection{The Inverse}
The inverse of a matrix $A$ is noted $A^{-1}$ and is the unique matrix
such that:
\begin{equation}
  A^{-1}A=I=AA^{-1}
\end{equation}
Not all square matrices are invertible. In addition, assuming
$A, B \in \mathbb{R}^{n \times n}$ are non-singular:
\begin{enumerate}
  \item $(A^{-1})^{-1} = A$
  \item $(AB)^{-1} = B^{-1}A^{-1}$
  \item $(A^{-1})^T = (A^T)^{-1}$
\end{enumerate}
\subsubsection{The Determinant}
The determinant of a square matrix $A\in\mathbb{R}^{n\times n}$, noted $|A|$
or $\textrm{det}(A)$ is expressed recursively in terms of
$A_{\backslash i, \backslash j}$, which is the matrix $A$ without
its $i$-th row and $j$-th column, as follows:
\begin{equation}
  \textrm{det}(A)=|A|=\sum_{j=1}^n(-1)^{i+j}A_{i,j}|A_{\backslash i,\backslash j}|
\end{equation}
Remark that $A$ is invertible if and only if $|A| \not= 0$. Also, $|AB|=|A||B|$
and $|A^T|=|A|$.
\subsection{Matrix Properties}
\subsubsection{Norms}
A norm of a vector $x$ is any function $f:\mathbb{R}^{n} \rightarrow \mathbb{R}$
that satisfies 4 properties:
\begin{enumerate}
  \item Non-negativity: For all $x \in \mathbb{R}^n$, $f(x) \geq 0$
  \item Definiteness: $f(x)=0$ if and only if $x=0$
  \item Homogeneity: For all $x \in \mathbb{R}^n$, $t \in \mathbb{R}$,
    $f(tx) = |t| f(x)$
  \item Triangle Inequality: For all $x, y \in \mathbb{R}^n$, $f(x+y) \leq f(x) + f(y)$
\end{enumerate}
However, most norms used come from the family of $\ell_p$ norms:
\begin{equation}
  \ell_p = ||x||_p = \left( \sum_{i=1}^n x_i^p \right)^{\frac{1}{p}}
\end{equation}
The $p$-norm is used in Holder's inequality. The Manhattan norm, used in
LASSO regularization, is:
\begin{equation}
  \ell_1 = ||x||_1 = \sum_{i=1}^n |x_i|
\end{equation}
The euclidean norm, $\ell_2$ is used in ridge regularization and
distance measures:
\begin{equation}
  \ell_2 = || x ||_2 = \sqrt{\sum_{i=1}^n x_i^2}
\end{equation}
Finally, the infinity norm is used in uniform convergence:
\begin{equation}
  \ell_{\infty} = || x ||_{\infty} = \underset{i}{\textrm{max }}|x_i|
\end{equation}
The Frobenius norm of a matrix is analogous to the $\ell_2$ norm of a vector:
\begin{equation}
  ||A||_F = \sqrt{\sum_{ij}A^2_{ij}}
\end{equation}
The dot product of two vectors can be expressed in terms of norms:
\begin{equation}
  x^T y = ||x||_2 ||y||_2 \cos \theta
\end{equation}
where $\theta$ is the angle between vectors $x$ and $y$.
\subsubsection{Linear Dependence and Rank}
A set of vectors $\{x_1, x_2, \hdots, x_n\} \subset \mathbb{R}^m$ is linearly
independent if no vector can be represented as a linear combination of the
remaining vectors. Conversely, if one vector in the set can be represented
as a linear combination of the remaining vectors, then the vectors are
said to be linearly dependent. Formally, for scalar values
$\alpha_1, \hdots, \alpha_{n-1} \in \mathbb{R}$:
\begin{equation}
  x_n = \sum_{i=1}^{n-1} \alpha_i x_i
\end{equation}
The rank of a given matrix $A$ is noted $\operatorname{rank}(A)$
and is the dimension of the vector space generated by its columns.
This is equivalent to the maximum number of linearly independent columns of
$A$. If $\operatorname{rank}(A) = \min(m,n)$, then $A$ is said to be full rank.m
\subsubsection{Span, Range, and Nullspace}
The span of a set of vectors $\{x_1, x_2, \hdots, x_n\}$ is the set of all
vectors that can be expressed as a linear combination of
$\{x_1, x_2, \hdots, x_n\}$. Formally,
\begin{equation}
  \operatorname { span } \left( \left\{ x _ { 1 } , \ldots x _ { n } \right\} \right) = \left\{ v : v = \sum _ { i = 1 } ^ { n } \alpha _ { i } x _ { i } , \quad \alpha _ { i } \in \mathbb { R } \right\}
\end{equation}
The projection of a vector $y \in \mathbb{R}^n$ onto the span of
$\{x_1, x_2, \hdots, x_n\}$ is the vector
$v \in \operatorname { span } \left( \left\{ x _ { 1 } , \ldots x _ { n } \right\} \right)$
such that $v$ is as close as possible to $y$ as measured by the euclidean norm:
\begin{equation}
  \operatorname { Proj } \left( y ; \left\{ x _ { 1 } , \ldots x _ { n } \right\} \right) = \operatorname { argmin } _ { v \in \operatorname { span } \left( \left\{ x _ { 1 } , \ldots , x _ { n } \right\} \right) } \| y - v \| _ { 2 }
\end{equation}
The range of a matrix $A\in \mathbb{R}^{m \times n}$ is the span of the columns
of $A$:
\begin{equation}
  \mathcal { R } ( A ) = \left\{ v \in \mathbb { R } ^ { m } : v = A x , x \in \mathbb { R } ^ { n } \right\}
\end{equation}
The nullspace of a matrix $A \in \mathbb{R}^{m \times n}$ is the set of all
vectors that equal $0$ when multiplied by $A$:
\begin{equation}
  \mathcal { N } ( A ) = \left\{ x \in \mathbb { R } ^ { n } : A x = 0 \right\}
\end{equation}
\subsubsection{Symmetric Matrices}
A square matrix $A \in \mathbb{R}^{n \times n}$ is symmetric if $A=A^T$. It
is anti-symmetric if $A=-A^T$. For any matrix $A\in \mathbb{R}^{n \times n}$
the matrix $A+A^T$ is symmetric and the matrix $A-A^T$ is anti-symmetric.
From this, any square matrix $A \in \mathbb{R}^{n \times n}$ can be represented
as a sum of a symmetric matrix and an anti-symmetric matrix:
\begin{equation}
  A=\underbrace{\frac{A+A^T}{2}}_{\textrm{Symmetric}}+\underbrace{\frac{A-A^T}{2}}_{\textrm{Antisymmetric}}
\end{equation}
\subsubsection{Positive Semidefinite Matrices}
Given a square matrix $A \in \mathbb{R}^{n \times n}$ and a vector
$x \in \mathbb{R}^n$, the scalar value $x^TAx$ is called the quadratic form:
\begin{equation}
  x^TAx = \sum_{i=1}^n \sum_{j=1}^n A_{ij} x_i x_j
\end{equation}
A symmetric matrix $A$ is:
\begin{enumerate}
  \item Positive definite (PD) if for all nonzero vectors, $x^TAx > 0$
  \item Positive semidefinite (PSD) if for all nonzero vectors, $x^TAx \geq 0$
  \item Negative definite (ND) if for all nonzero vectors, $x^TAx < 0$
  \item Negative semidefinite (NSD) if for all nonzero vectors, $x^TAx \leq 0$
  \item Indefinite if it is neiter PSD nor NSD, i.e. if there exists a $x_1, x_2$
    such that $x^T_1Ax_1 > 0$ and $x^T_2Ax_2 < 0$
\end{enumerate}
Given any matrix $A \in \mathbb{R}^{m \times n}$, the gram matrix $G = A^T A$
is always PSD. If $m \geq n$ and $A$ is full rank, then $G$ is PD.
\subsubsection{Eigendecomposition}
Given a square matrix $A \in \mathbb{R}^{n \times n}$, we say that
$\lambda \in \mathbb{C}$ is an eigenvalue of $A$ and $v \in \mathbb{C}$ is the
corresponding eigenvector if
\begin{equation}
  Av = \lambda v, \quad v \not= 0
\end{equation}
The trace of $A$ is equal to the sum of its eigenvalues:
\begin{equation}
  \operatorname{tr}A = \sum_{i=1}^n \lambda_i
\end{equation}
The determinant of $A$ is equal to the product of its eigenvalues:
\begin{equation}
  |A| = \prod_{i=1}^n \lambda_i
\end{equation}
Suppose matrix $A$ has $n$ linearly independent eigenvectors
$\{v_1, \hdots, v_n\}$ with corresponding eigenvalues
$\{\lambda_1, \hdots, \lambda_n\}$. Let $V$ be a matrix with one eigenvalue
per column: $V = \left[ v_1, \hdots, v_n \right]$. Let
$\Lambda =\textrm{diag}(\lambda_1,\hdots,\lambda_n)$. Then the eigendecomposition
of $A$ is given by:
\begin{equation}
  A = V \Lambda V^{-1}
\end{equation}
For when $A$ is symmetric, then the eigenvalues of $A$ are real, the
eigenvectors of $A$ are orthonormal, and hence $V$ is an orthogonal matrix
(henceforth renamed $U$). Hence $A = U \Lambda U^T$.
A matrix whose eigenvalues are all positive is called positive definite.
A matrix whose eigenvalues are all positive or zero valued is called positive
semidefinite. Likewise, if all eigenvalues are negative, the matrix is
negative definite, and if all eigenvalues are negative or zero valued,
it is negative semidefinite. In addition, assuming sorted eigenvalues,
for the following optimization problem:
\begin{equation}
  \max _ { x \in \mathbb { R } ^ { n } } x ^ { T } A x \quad \text { subject to } \| x \| _ { 2 } ^ { 2 } = 1
\end{equation}
The solution is $v_1$ the eigenvector corresponding to $\lambda_1$. For
the minimization problem:
\begin{equation}
  \min _ { x \in \mathbb { R } ^ { n } } x ^ { T } A x \quad \text { subject to } \| x \| _ { 2 } ^ { 2 } = 1
\end{equation}
the optimal solution for $x$ is $v_n$, the eigenvector corresponding to
eigenvalue $\lambda_n$.
\subsubsection{Singlular Value Decomposition}
The singular value decomposition provides a way to factorize a $m \times n$
matrix $A$ into singular vectors and singular values. It is defined as:
\begin{equation}
  A = U D V^T
\end{equation}
Suppose $A \in \mathbb{R}^{m \times n}$ matrix. Then
$U \in \mathbb{R}^{m \times m}$, $D \in \mathbb{R}^{m \times n}$,
$ V \in \mathbb{R}^{n \times n}$ matrices. The matricies $U$ and $V$ are
orthogonal, and matrix $D$ is diagonal. The elements along the diagonal of $D$
are known as the singular values of matrix $A$. The columns of $U$ are known
as the left-singular vectors while the columns of $V$ are the right-singular
vectors. The left-singular vectors of $A$ are the eigenvectors of $AA^T$.
The right-singular vectors of $A$ are the eigenvectors of $A^TA$. We can use
SVD to partially generalize matrix inversion to nonsquare matrices.
\subsubsection{The Moore-Penrose Pseudoinverse}
Matrix inversion is not defined for matrices that are not square.
Note that for nonsingular $A$:
\begin{equation}
  A A^{-1}A = A
\end{equation}
However, if the inverse is not defined, we seek to find a matrix $A^{+}$ such
that:
\begin{equation}
  A A^{+}A = A
\end{equation}
The moore-penrose pseudoinverse $A^+$ is defined as follows:
\begin{equation}
  A^+ = \lim_{\alpha \to 0} (A^TA + \alpha I)^{-1}A^T
\end{equation}
Practical algorithms use the singular value decomposition of $A$ such that:
\begin{equation}
  A^+ = V D^+ U^T
\end{equation}
where $U, D, V$ are from the SVD of $A$, and the pseudoinverse of $D^+$ is
obtained by taking the reciprocal of the nonzero diagonal elements of $D$.
If $A$ has more columns than rows, then using the pseudoinverse to solve a
linear equation $Ax=y$ provides one of many solutions, but provides $x = A^+ y$
with minimal euclidean norm $||x||_2$. When $A$ has more rows than columns,
the pseudoinverse gives us the $x$ for which $Ax$ is as close as possible
to $y$, i.e. minimizing $||Ax-y||_2$.
\subsection{Matrix Calculus}
\subsubsection{The Gradient}
Let $f:\mathbb{R}^{m\times n}\rightarrow\mathbb{R}$ be a function and
$A \in \mathbb{R}^{m \times n}$ be a matrix. The gradient of $f$ with respect to
$A$ is a $m \times n$ matrix noted as $\nabla_A f(A)$ such that:
\begin{equation}
  \nabla _ { A } f ( A ) \in \mathbb { R } ^ { m \times n } = \left(
  \begin{array} { c c c c } { \frac { \partial f ( A ) } { \partial A _ { 11 } } } & { \frac { \partial f ( A ) } { \partial A _ { 12 } } } & { \cdots } & { \frac { \partial f ( A ) } { \partial A _ { 1 } } } \\ { \frac { \partial f ( A ) } { \partial A _ { 21 } } } & { \frac { \partial f ( A ) } { \partial A _ { 22 } } } & { \cdots } & { \frac { \partial f ( A ) } { \partial A _ { 2 n } } } \\ { \vdots } & { \vdots } & { \ddots } & { \vdots } \\ { \frac { \partial f ( A ) } { \partial A _ { m 1 } } } & { \frac { \partial f ( A ) } { \partial A _ { m 2 } } } & { \cdots } & { \frac { \partial f ( A ) } { \partial A _ { m n } } } \end{array} \right)
\end{equation}
Or compactly for each $ij$ entry:
\begin{equation}
  \nabla _ { A } f ( A ) _ { i j } = \frac { \partial f ( A ) } { \partial A _ { i j } }
\end{equation}
However, the gradient of a vector $x \in \mathbb{R}^n$ is:
\begin{equation}
  \nabla _ { x } f ( x ) = \left( \begin{array} { c } { \frac { \partial f ( x ) } { \partial x _ { 1 } } } \\ { \frac { \partial f ( x ) } { \partial x _ { 2 } } } \\ { \vdots } \\ { \frac { \partial f ( x ) } { \partial x _ { n } } } \end{array} \right)
\end{equation}
\subsubsection{The Hessian}
Let $f:\mathbb{R}^{n}\rightarrow\mathbb{R}$ be a function and
$x \in \mathbb{R}^{n}$ be a vector. The hessian of $f$ with respect to
$x$ is a $n \times n$ symmetric matrix noted as $H = \nabla_x^2 f(x)$ such that:
\begin{equation}
  \nabla _ { x } ^ { 2 } f ( x ) \in \mathbb { R } ^ { n \times n } = \left( \begin{array} { c c c c } { \frac { \partial ^ { 2 } f ( x ) } { \partial x _ { 1 } ^ { 2 } } } & { \frac { \partial ^ { 2 } f ( x ) } { \partial x _ { 2 } } } & { \cdots } & { \frac { \partial ^ { 2 } f ( x ) } { \partial x _ { 1 } \partial x _ { n } } } \\ { \frac { \partial ^ { 2 } f ( x ) } { \partial x _ { 2 } \partial x _ { 1 } } } & { \frac { \partial ^ { 2 } f ( x ) } { \partial x _ { 2 } ^ { 2 } } } & { \cdots } & { \frac { \partial ^ { 2 } f ( x ) } { \partial x _ { 2 } \partial x _ { n } } } \\ { \vdots } & { \vdots } & { \ddots } & { \vdots } \\ { \frac { \partial ^ { 2 } f ( x ) } { \partial x _ { n } \partial x _ { 1 } } } & { \frac { \partial ^ { 2 } f ( x ) } { \partial x _ { n } \partial x _ { 2 } } } & { \cdots } & { \frac { \partial ^ { 2 } f ( x ) } { \partial x _ { n } ^ { 2 } } } \end{array} \right)
\end{equation}
Or compactly:
\begin{equation}
  \nabla _ { x } ^ { 2 } f ( x ) _ { i j } = \frac { \partial ^ { 2 } f ( x ) } { \partial x _ { i } \partial x _ { j } }
\end{equation}
Note that the hessian is only defined when $f(x)$ is real-valued.
\subsubsection{Gradient Properties}
For matrices $A, B, C$ and vectors $x, b$:
\begin{enumerate}
  \item $\nabla_x b^Tx = b$
  \item $\nabla_x x^TAx = 2Ax$ (if $A$ symmetric)
  \item $\nabla^2_x x^TAx = 2A$ (if $A$ symmetric)
  \item $\nabla_A\textrm{tr}(AB)=B^T$
  \item $\nabla_{A^T}f(A)=\left(\nabla_Af(A)\right)^T$
  \item $\nabla_A\textrm{tr}(ABA^TC)=CAB+C^TAB^T$
  \item $\nabla_A|A|=|A|(A^{-1})^T$
\end{enumerate}
