\section{Probability and Statistics}
\subsection{Basics}
The set of all possible outcomes of an experiment is known as the sample space
and denoted by $S$. Any subset $E$ of the sample space is known as an event.
An event is a set consisting of possible outcomes of the experiment.
\subsubsection{Axioms of Probability}
For each event $E$, we denote $P(E)$ as the probability of event $E$ occuring.
$P(E)$ satisfies the following properties:
\begin{enumerate}
  \item Every probability is between 0 and 1 included:
    \begin{equation} 0 \leq P(E) \leq 1 \end{equation}
  \item The probability that at least one event in the sample space will occur
  is $1$: \begin{equation} P(S) = 1 \end{equation}
  \item For any sequence of mutually exclusive events $E_1,\hdots,E_n$ we have:
    \begin{equation} P\left(\bigcup_{i=1}^nE_i\right)=\sum_{i=1}^nP(E_i) \end{equation}
\end{enumerate}
\subsubsection{Permutation}
A permutation is an arrangement of $r$ objects from a pool of $n$ objects, in
a given order. The number of such arrangements is given by $P(n, r)$:
\begin{equation} P(n, r) = \frac{n!}{(n-r)!} \end{equation}
\subsubsection{Combination}
A combination is an arrangement of $r$ objects from a pool of $n$ objects, where
order does not matter. The number of such arrangements is given by $C(n, r)$:
\begin{equation} C(n, r) = \frac{P(n, r)}{r!} = \frac{n!}{r! (n-r)!} \end{equation}
Note that for $0 \leq r \leq n$ we have $P(n, r) \geq C(n, r)$.
\subsection{Conditional Probability}
Let $B$ be an event with non-zero probability, the conditional proability of
any event $A$ given $B$ is:
\begin{equation} P(A | B) = \frac{P(A \cap B)}{P(B)} \end{equation}
\subsubsection{Bayes Rule}
For events $A, B$ such that $P(B) > 0$, we have:
\begin{equation} P(A | B) = \frac{P(B|A) P(A)}{P(B)} \end{equation}
From Bayes rule, we have:
\begin{equation} P(A \cap B) = P(A|B)P(B) = P(A)P(B|A) \end{equation}
Let $\{ A_i, i \in [1, n] \}$ be such that for all $i$, $A_i \not= \varnothing$.
We say that $\{A_i\}$ is a partition if we have:
\begin{equation} \forall i\neq j, A_i\cap A_j=\emptyset\quad\mbox{ and }\quad\bigcup_{i=1}^nA_i=S \end{equation}
Remark that for any event $B$ in the sample space, we have:
\begin{equation} P(B)=\sum_{i=1}^n P(B|A_i)P(A_i) \end{equation}
Let $\{ A_i, i \in [1, n] \}$ be a partition of the sample space, we can
extend bayes rule as:
\begin{equation} P(A_k|B)=\frac{P(B|A_k)P(A_k)}{\displaystyle\sum_{i=1}^nP(B|A_i)P(A_i)} \end{equation}
\subsubsection{Independence}
Two events $A$ and $B$ are independent if and only if we have:
\begin{equation} P(A \cap B) = P(A) P(B) \end{equation}
\subsection{Random Variables}
A random variable $X$ is a function that maps every element in a sample space
to a real line.
\subsubsection{Cumulative Distribution Function (CDF)}
The cumulative distribution function $F$, which is monotonically non-decreasing
and is such that $\lim_{x \to -\infty}F(X) = 0$ and $\lim_{x \to \infty}F(X) = 1$
is defined as:
\begin{equation} F(x) = P(X \leq x) \end{equation}
In addition:
\begin{equation} P(a < X \leq b) = F(b) - F(a) \end{equation}
\subsubsection{Probability Density Function (PDF)}
The probability density function is the derivative of the CDF.
It has the following properties:
\begin{enumerate}
  \item $f(x) \geq 0$
  \item $\int^{\infty}_{-\infty} f(x) = 1$
  \item $\int_{x \in A} f(x) dx = P(X \in A)$
\end{enumerate}
\subsubsection{Discrete PDF/CDF}
If $X$ is discrete, by denoting $f$ as the PDF and $F$ as the CDF, we have:
\begin{equation}
  F(X) = \sum_{x_i \leq x} P(X = x_i)
\end{equation}
\begin{equation}
  f(x_j) = P(X = x_j)
\end{equation}
And the following properties for the PDF:
\begin{equation}
  0 \leq f(x_j) \leq 1
\end{equation}
\begin{equation}
  \sum_j f(x_j) = 1
\end{equation}
\subsubsection{Continuous PDF/CDF}
If $X$ is continuous, by denoting $f$ as the PDF and $F$ as the CDF, we have:
\begin{equation}
  F(X) = \int_{-\infty}^x f(y) dy
\end{equation}
\begin{equation}
  f(x) = \frac{dF}{dx}
\end{equation}
And the following properties for the PDF:
\begin{equation}
  f(x) \geq 0
\end{equation}
\begin{equation}
  \int_{-\infty}^{\infty} f(x) dx = 1
\end{equation}
\subsubsection{Expectation}
The expected value of a random variable, also known as the mean value or
first moment, is denoted as $\mathbb { E }[X]$ or $\mu$. It is the value obtained by
averaging the results of a random variable.
We use $\mbox{(D)}$ for discrete, $\mbox{(C)}$ for continuous.
\begin{equation}
  \mbox{(D)}\quad \mathbb { E }[X]=\sum_{i=1}^nx_if(x_i)\quad\quad\mbox{and}\quad\mbox{(C)}\quad \mathbb { E }[X]=\int_{-\infty}^{+\infty}xf(x)dx
\end{equation}
The expected value of a function of a random variable $g(X)$ is:
\begin{equation}
  \mbox{(D)}\quad \mathbb { E }[g(X)]=\sum_{i=1}^ng(x_i)f(x_i)\quad\quad\mbox{and}\quad\mbox{(C)}\quad \mathbb { E }[g(X)]=\int_{-\infty}^{+\infty}g(x)f(x)dx
\end{equation}
The $k$-th moment, noted $\mathbb { E }[X^k]$ is the value of $X^k$ that we expect to observe
on average on infinitely many trials. The $k$-th moment is a case of the previous
definition with $g \,:\, X \mapsto X^k$.
\begin{equation}
  \mbox{(D)}\quad \mathbb { E }[X^k]=\sum_{i=1}^nx_i^kf(x_i) \quad\quad\mbox{and}\quad\mbox{(C)}\quad \mathbb { E }[X^k]=\int_{-\infty}^{+\infty}x^kf(x)dx
\end{equation}
A characteristic function $\psi(\omega)$ is derived from a probability density
function $f(x)$ and is defined as:
\begin{equation}
  \mbox{(D)}\quad \psi(\omega)=\sum_{i=1}^nf(x_i)e^{i\omega x_i} \quad\quad\mbox{and}\quad\mbox{(C)}\quad \psi(\omega)=\int_{-\infty}^{+\infty}f(x)e^{i\omega x}dx
\end{equation}
Remark that $e^{i\omega x} = \cos(\omega x)+ i \sin(\omega x)$. The $k$-th moment
can also be computed with the characteristic function as:
\begin{equation}
  \mathbb { E }[X^k]=\frac{1}{i^k}\left[\frac{\partial^k\psi}{\partial\omega^k}\right]_{\omega=0}
\end{equation}
\subsubsection{Variance and Standard Deviation}
The variance of a random variable, often noted $\mbox{Var}(X)$ or $\sigma^2$,
is a measure of the spread of its distribution function.
It is determined as follows:
\begin{equation}
  \mbox{Var}[X]=\mathbb { E }[(X-\mathbb { E }[X])^2]=\mathbb { E }[X^2]-\mathbb { E }[X]^2
\end{equation}
The standard deviation of a random variable, often noted $\sigma$, is a
measure of the spread of its distribution function which is
compatible with the units of the actual random variable.
It is determined as follows:
\begin{equation}
  \sigma = \sqrt{\mbox{Var}[X]}
\end{equation}
Note that the variance for any constant $a$ is $\mbox{Var}[a]= 0$, and
$\mbox{Var}[af(X)]=a^2\mbox{Var}[f(X)]$.
\subsection{Discrete Random Variables}
\subsubsection{Bernoulli}
For $X\sim Bernoulli(p)$, we define a binary event with a probability of $p$
for a true event, and a false event with probability of $q=1-p$.
\begin{equation}
  P(X=x) = \begin{cases} q=1-p & \textrm{if } x=0 \\ p & \textrm{if } x=1 \end{cases}
\end{equation}
It can also be expressed as:
\begin{equation}
  f(x; p) = p^k (1-p)^{1-k} \quad \mbox{for } k\in \{0, 1\}
\end{equation}
Other properties:
\begin{equation}
  \mathbb { E }[X] = p
\end{equation}
\begin{equation}
  \mbox{Var}[X] = pq = p (1-p)
\end{equation}
\begin{equation}
  \psi(\omega) = (1-p) + p e^{i\omega}
\end{equation}
\subsubsection{Binomial}
For $X\sim Binomial(n, p)$, the number of true events in $n$ independent
experiments, with true probability of $p$, false probability of $q=1-p$.
\begin{equation}
  \displaystyle P(X=x)=\displaystyle\binom{n}{x} p^x(1-p)^{n-x}
\end{equation}
Other properties:
\begin{equation}
  \mathbb { E }[X] = np
\end{equation}
\begin{equation}
  \mbox{Var}[X] = npq = np(1-p)
\end{equation}
\begin{equation}
  \psi(\omega) = (pe^{i\omega}+(1-p))^n
\end{equation}
\subsubsection{Geometric}
For $X\sim Geometric(p)$, is the number of experiments with true
probability of $p$ until the first true event (number of trials to get one success).
\begin{equation}
  \displaystyle P(X=x)=p(1-p)^{x-1}
\end{equation}
Other properties:
\begin{equation}
  \mathbb { E }[X] = \frac{1}{p}
\end{equation}
\begin{equation}
  \mbox{Var}[X] = \frac{1-p}{p^2}
\end{equation}
\begin{equation}
  \psi(\omega) = \frac{pe^{i \omega}}{1-(1-p)e^{i \omega}}
\end{equation}
\subsubsection{Poisson}
For $X\sim Poisson(\lambda)$, for $\lambda > 0$, a probability distribution
over the nonnegative integers
used for modeling the frequency of rare events.
\begin{equation}
  \displaystyle P(X=x)=\frac{\lambda^x}{x!}e^{-\lambda}
\end{equation}
Other properties:
\begin{equation}
  \mathbb { E }[X] = \lambda
\end{equation}
\begin{equation}
  \mbox{Var}[X] = \lambda
\end{equation}
\begin{equation}
  \psi(\omega) = e^{\lambda(e^{i\omega}-1)}
\end{equation}
\subsection{Continuous Random Variables}
\subsubsection{Uniform}
For $X\sim Uniform(a,b)$, we have equal probability density to every value
between $a$ and $b$.
\begin{equation}
  f(x) = \begin{cases} \frac{1}{b-a} & \textrm{if } a \leq x \leq b \\ 0 & \textrm{otherwise} \end{cases}
\end{equation}
Other properties:
\begin{equation}
  \mathbb { E }[X] = \frac{a+b}{2}
\end{equation}
\begin{equation}
  \mbox{Var}[X] = \frac{(b-a)^2}{12}
\end{equation}
\begin{equation}
  \psi(\omega) = \displaystyle\frac{e^{i\omega b}-e^{i\omega a}}{(b-a)i\omega}
\end{equation}
\subsubsection{Exponential}
For $X\sim Exponential(\lambda)$, $\lambda > 0$, is the
decaying probability density over the nonnegative reals.
\begin{equation}
  f(x) = \begin{cases} \lambda e^{-\lambda x} & \textrm{if } x \geq 0 \\ 0 & \textrm{otherwise} \end{cases}
\end{equation}
Other properties:
\begin{equation}
  \mathbb { E }[X] = \frac{1}{\lambda}
\end{equation}
\begin{equation}
  \mbox{Var}[X] = \frac{1}{\lambda^2}
\end{equation}
\begin{equation}
  \psi(\omega) = \displaystyle\frac{1}{1-\frac{i\omega}{\lambda}}
\end{equation}
\subsubsection{Gaussian (Normal)}
For $X\sim Normal(\mu, \sigma)$, denoted also $X \sim \mathcal{N}(\mu, \sigma)$.
\begin{equation}
  \displaystyle f(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2}
\end{equation}
Other properties:
\begin{equation}
  \mathbb { E }[X] = \mu
\end{equation}
\begin{equation}
  \mbox{Var}[X] = \sigma^2
\end{equation}
\begin{equation}
  \psi(\omega) = e^{i\omega\mu-\frac{1}{2}\omega^2\sigma^2}
\end{equation}
\subsection{Jointly Distributed Random Variables}
The joint probability distribution of two random variables $X$ and $Y$, denoted
as $f_{XY}$ is defined as:
\begin{equation}
  \mbox{(D)}\quad f_{XY}(x_i,y_j)=P(X=x_i\mbox{ and }Y=y_j)
\end{equation}
\begin{equation}
  \mbox{(C)}\quad f_{XY}(x,y)\Delta x\Delta y=P(x\leqslant X\leqslant x+\Delta x\mbox{ and }y\leqslant Y\leqslant y+\Delta y)
\end{equation}
Again, denote $\mbox{(D)}$ as the discrete case, and $\mbox{(C)}$ as the
continuous case.
\subsubsection{Marginal Density}
The marginal density for a random variable $X$ is:
\begin{equation}
  \mbox{(D)}\quad f_X(x_i)=\sum_{j}f_{XY}(x_i,y_j) \quad\quad\mbox{and}\quad\mbox{(C)}\quad f_X(x)=\int_{-\infty}^{+\infty}f_{XY}(x,y)dy
\end{equation}
\subsubsection{Cumulative Distribution}
The cumulative distribution $F_{XY}$ is:
\begin{equation}
  \mbox{(D)}\quad F_{XY}(x,y)=\sum_{x_i\leqslant x}\sum_{y_j\leqslant y}f_{XY}(x_i,y_j) \quad\quad\mbox{and}\quad\mbox{(C)}\quad F_{XY}(x,y)=\int_{-\infty}^x\int_{-\infty}^yf_{XY}(x',y')dx'dy'
\end{equation}
\subsubsection{Conditional Density}
The conditional density of $X$ with respect to $Y$, denoted $f_{X|Y}$ is
defined as:
\begin{equation}
  f_{X|Y} = \frac{f_{XY}(x, y)}{f_Y(y)}
\end{equation}
\subsubsection{Independence}
Two random variables $X$ and $Y$ are independent if:
\begin{equation}
  f_{XY}(x,y) = f_X(x)f_Y(y)
\end{equation}
\subsubsection{Expectation}
Given two random variables $X,Y$ and $g : \mathbb{R}^2 \to \mathbb{R}$ is a
function of these two variables. Then the expected value of $g$ is:
\begin{equation}
  \mbox{(D)}\quad\mathbb{E}[g(X, Y)] = \sum_i \sum_j g(x_i, y_i) f(x_i, y_i) \quad\quad\mbox{and}\quad\mbox{(C)}\quad\mathbb{E}[g(X, Y)] = \int_{-\infty}^{\infty}\int_{-\infty}^{\infty} g(x_i, y_i) f(x_i, y_i) dy dx
\end{equation}
\subsubsection{Covariance}
The covariance of two random variables $X$ and $Y$, denoted $\sigma^2_{XY}$ or
as $\mbox{Cov}[X, Y]$ is defined as:
\begin{equation}
  \mbox{Cov}[X,Y]\triangleq\sigma_{XY}^2=\mathbb{E}[(X-\mu_X)(Y-\mu_Y)]=\mathbb{E}[XY]-\mu_X\mu_Y
\end{equation}
Where $\mu_X = \mathbb{E}[X]$ and $\mu_Y = \mathbb{E}[Y]$ respectively. If $X$
and $Y$ are independent, the covariance is $0$.
\begin{equation}
  \mbox{Var}[X + Y] = \mbox{Var}[X] + \mbox{Var}[Y] + 2 \mbox{Cov}[X, Y]
\end{equation}
\subsubsection{Correlation}
By noting $\sigma_X$, $\sigma_Y$ as the standard deviations of $X$ and $Y$, we
define the correlation between the random variables $X$ and $Y$ as $\rho_{XY}$:
\begin{equation}
  \rho_{XY} = \frac{\sigma_{XY}^2}{\sigma_{X}\sigma_{Y}}
\end{equation}
Correlation for $X, Y$ is $\rho_{XY} \in [-1, 1]$. If $X,Y$ independent, then
$\rho_{XY}=0$
\subsection{Parameter Estimation}




\subsubsection{Definitions}
\subsubsection{Bias}
\subsubsection{Mean and Central Limit Theorem}
\subsubsection{Variance}







\subsection{Probability Bounds and Inequalities}
This section looks at various bounds that define how likely a random variable
is to be close to its expectation.
\subsubsection{Markov}
Let $X \geq 0$ be a non-negative random variable. Then for all $a \geq 0$:
\begin{equation}
  P(X \geq a) \leq \frac{\mathbb{E}[X]}{a}
\end{equation}
\subsubsection{Chebyshev}
Let $X$ be any random variable with finite expected value $\mu = \mathbb{E}[X]$
and finite non-zero variance $\sigma^2$.
Then for all $k > 0$:
\begin{equation}
  P(|X - \mathbb{E}[X]| \geq k \sigma) \leq \frac{1}{k^2}
\end{equation}
\subsubsection{Chernoff}
Recall that the moment generating function for a random variable $X$ is:
\begin{equation}
  M_X(\lambda) := \mathbb{E}[e^{\lambda X}]
\end{equation}
Then the chernoff bound for a random variable $X$, obtained by applying the
markov inequality to $e^{\lambda X}$, for every $\lambda > 0$:
\begin{equation}
  P(X \geq a) = P(e^{\lambda X} \geq e^{\lambda a}) \leq \frac{\mathbb{E}[e^{\lambda X}]}{e^{\lambda a}}
\end{equation}
For the multiplicative chernoff bound, suppose $X_1, \hdots, X_n$ are independent
random variables taking values in $\{0, 1\}$. Let $X$ denote the sum, $\mu=\mathbb{E}[X]$
denote the sum's expected value. Then for any $0 < \delta < 1$,
\begin{equation}
  P(X > (1+\delta) \mu ) < \left( \frac{e^{\delta}}{(1+\delta)^{(1+\delta)}} \right)^{\mu} \leq e^{-\frac{\delta^2 \mu}{3}}
\end{equation}
\begin{equation}
  P(X < (1-\delta) \mu ) < \left( \frac{e^{-\delta}}{(1-\delta)^{(1-\delta)}} \right)^{\mu} \leq e^{-\frac{\delta^2 \mu}{2}}
\end{equation}
For $\delta \geq 0$,
\begin{equation}
  P(X \geq (1+\delta) \mu ) \leq e^{-\frac{\delta^2 \mu}{2+\delta}}
\end{equation}
\begin{equation}
  P(X \leq (1-\delta) \mu ) \leq e^{-\frac{\delta^2 \mu}{2}}
\end{equation}
\subsubsection{Hoeffding}
Let $X_1, \hdots, X_n$ be independent random variables such that $a_i \leq X_i \leq b_i$.
The sum of these variables $S_n = \sum_i X_i$, then the Hoeffding
inequality is:
\begin{equation}
  P\left(S_n - \mathbb{E}\left[S_n\right] \geq t \right) \leq \exp\left(-\frac{2t^2}{\sum_i(b_i-a_i)^2}\right)
\end{equation}
\begin{equation}
  P\left(\left|S_n - \mathbb{E}\left[S_n\right]\right| \geq t \right) \leq 2\exp\left(-\frac{2t^2}{\sum_i(b_i-a_i)^2}\right)
\end{equation}
The hoeffding lemma states that for a real-valued random variable $X$ with expected
value $\mathbb{E}[X] = 0$ and such that $a \leq X \leq b$, then for all
$\lambda \in \mathbb{R}$ we have,
\begin{equation}
  \mathbb{E}[e^{\lambda X}] \leq \exp\left(\frac{\lambda^2(b-a)^2}{8}\right)
\end{equation}
